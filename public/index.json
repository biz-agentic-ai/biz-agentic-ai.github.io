[{"content":"\rGoogle Colab에서 실습하기\r데이터 레이크가 늪이 되는 과정 데이터 레이크에 파일을 쏟아놓고 바로 분석하려는 팀이 있다. 처음엔 빠르다. CSV 올리고 SQL 한 줄이면 결과가 나온다.\n3개월 지나면 상황이 달라진다. 누가 올린 파일인지 모른다. 원본인지 정제된 건지 구분이 안 된다. 같은 매출 테이블인데 부서마다 숫자가 다르다. 데이터 늪(data swamp)이라고 부르는 상태다.\n원인은 단순하다. 원본과 가공물이 같은 공간에 섞여 있기 때문이다. 레이어를 나누면 이 문제가 풀린다.\nBronze, Silver, Gold 메달리온 아키텍처는 데이터를 세 개의 레이어로 나눈다. Databricks가 이름을 붙여서 널리 퍼졌지만, 개념 자체는 전통 DW의 레이어드 접근과 같다.\n소스 시스템 → [Bronze] → [Silver] → [Gold] → BI / 분석 원본 적재 정제·표준화 비즈니스 집계 Bronze 는 원본 그대로다. 소스 시스템에서 가져온 데이터를 변환 없이 저장한다. CSV든 JSON이든 API 응답이든 있는 그대로. 데이터 계보(lineage)의 출발점이다. 여기서 뭔가를 바꾸면 원본을 잃는다.\nSilver 는 정제와 표준화다. Bronze 데이터의 타입을 맞추고, 중복을 제거하고, 키를 통합한다. \u0026ldquo;분석에 쓸 수 있는 상태\u0026quot;로 만드는 레이어다. 비즈니스 로직은 아직 넣지 않는다.\nGold 는 비즈니스 관점의 집계다. 팩트 테이블, 차원 테이블, KPI 마트. 최종 사용자가 직접 쿼리하는 레이어다. DW 모델링 1편에서 다뤘던 스타스키마가 여기에 해당한다.\n각 레이어의 역할이 명확하다는 게 핵심이다. Bronze에서는 절대 변환하지 않는다. Silver에서는 비즈니스 로직을 넣지 않는다. Gold에서만 비즈니스 관점의 가공이 들어간다. 이 규칙이 깨지면 레이어를 나눈 의미가 없다.\n전통 DW 레이어와의 대응 DW 모델링 시리즈에서 Raw → Staging → Integration → Mart 구조를 다뤘다. 메달리온과 이름만 다르고 역할은 거의 같다.\n메달리온 전통 DW 하는 일 Bronze Raw / Staging 원본 적재, 변환 없음 Silver Integration (3NF / Data Vault) 정제, 표준화, 키 통합 Gold Mart (Star Schema) 비즈니스 집계, 분석용 전통 DW에서는 Staging과 Integration 사이에 ETL 서버가 무거운 변환을 처리했다. 메달리온은 ELT 패러다임이다. 일단 Bronze에 적재하고, DW 엔진 안에서 Silver와 Gold를 만든다. 변환을 별도 서버가 아니라 DW 엔진의 컴퓨팅 파워로 처리한다는 점이 다르다.\n이 시리즈의 실습 환경 시리즈 전체에서 사용할 도구는 세 가지다. 전부 무료이고, Google Colab에서 클라우드 계정 없이 바로 돌릴 수 있다.\n도구 역할 DuckDB 로컬 DW 엔진. Columnar Storage 기반이라 BigQuery/Snowflake와 같은 방식으로 동작한다 dbt-core + dbt-duckdb 변환 레이어. SQL로 Bronze → Silver → Gold를 정의한다 Soda Core 데이터 품질 검증. 레이어 간 품질 게이트를 건다 DuckDB를 고른 이유가 있다. 설치가 pip install 한 줄이면서, 실제 클라우드 DW와 동작 방식이 같다. Parquet, CSV를 네이티브로 읽고, SQL로 분석하고, Columnar Storage라 컬럼 기반 스캔이 된다. 로컬에서 돌리지만 클라우드 DW의 축소판이라고 보면 된다.\n환경 세팅 Colab 셀에서 아래를 실행하면 준비 끝이다.\n# 도구 설치 !pip install -q duckdb dbt-core dbt-duckdb import duckdb # DuckDB 데이터베이스 생성 conn = duckdb.connect(\u0026#39;warehouse.duckdb\u0026#39;) print(f\u0026#34;DuckDB {duckdb.__version__} 준비 완료\u0026#34;) 샘플 데이터 준비 시리즈에서 사용할 샘플은 간단한 이커머스 데이터다. 주문, 고객, 상품 세 테이블. DW 모델링 2편에서 다뤘던 구조와 같은 도메인이다.\n# Bronze 레이어: 원본 그대로 적재 conn.execute(\u0026#34;\u0026#34;\u0026#34; CREATE SCHEMA IF NOT EXISTS bronze; CREATE OR REPLACE TABLE bronze.orders AS SELECT * FROM read_csv_auto(\u0026#39;https://raw.githubusercontent.com/ dbt-labs/jaffle_shop/main/seeds/raw_orders.csv\u0026#39;); CREATE OR REPLACE TABLE bronze.customers AS SELECT * FROM read_csv_auto(\u0026#39;https://raw.githubusercontent.com/ dbt-labs/jaffle_shop/main/seeds/raw_customers.csv\u0026#39;); CREATE OR REPLACE TABLE bronze.payments AS SELECT * FROM read_csv_auto(\u0026#39;https://raw.githubusercontent.com/ dbt-labs/jaffle_shop/main/seeds/raw_payments.csv\u0026#39;); \u0026#34;\u0026#34;\u0026#34;) # 적재 확인 conn.execute(\u0026#34;SELECT count(*) as cnt FROM bronze.orders\u0026#34;).fetchdf() Bronze에 적재했다. CSV를 읽어서 DuckDB에 넣었을 뿐, 어떤 변환도 하지 않았다. 타입 캐스팅도 안 했고, 컬럼명도 바꾸지 않았다. 이게 Bronze의 원칙이다.\n# Bronze 데이터 확인 conn.execute(\u0026#34;SELECT * FROM bronze.orders LIMIT 5\u0026#34;).fetchdf() 여기서 바로 분석 쿼리를 던지고 싶은 유혹이 생긴다. 참아야 한다. Bronze 데이터를 직접 분석에 쓰면 3개월 뒤에 데이터 늪에 빠진다. 다음 글에서 Bronze를 Silver로 올리는 과정을 다룬다.\n왜 이렇게까지 나누는가 레이어를 나누면 느려지지 않냐는 질문을 받는다. 저장 공간도 더 쓰고, 변환 단계도 늘어나니까.\n맞다. 대신 세 가지를 얻는다.\n재처리가 가능하다. Silver 로직에 버그가 있으면 Bronze에서 다시 만들면 된다. 원본이 살아있으니까. Bronze 없이 Silver만 있으면 소스 시스템에서 다시 끌어와야 한다.\n문제 추적이 된다. Gold의 숫자가 이상하면 Silver를 보고, Silver가 이상하면 Bronze를 본다. 어느 레이어에서 문제가 생겼는지 특정할 수 있다.\n역할이 분리된다. 데이터 엔지니어는 Bronze→Silver를 책임지고, 분석 엔지니어는 Silver→Gold를 책임진다. 서로의 영역을 건드리지 않아도 된다.\n클라우드 환경에서 스토리지 비용은 거의 무시할 수 있는 수준이다. 레이어를 하나 더 두는 비용보다, 데이터 늪에 빠졌을 때의 비용이 훨씬 크다.\n다음 글에서는 Bronze 레이어를 본격적으로 다룬다. Full Load와 Incremental Load의 차이, 증분 적재의 기준 컬럼을 어떻게 잡는지.\nGoogle Colab에서 실습하기\r","permalink":"https://biz-agentic-ai.github.io/guides/etl-design/001-medallion-architecture/","summary":"Bronze, Silver, Gold. 데이터를 레이어별로 나눠서 적재하면 뭐가 달라지는가. DuckDB와 dbt로 직접 구성해 본다.","title":"1. 메달리온 아키텍처 - 데이터를 세 겹으로 쌓는 이유"},{"content":"\rGoogle Colab에서 실습하기\r원본을 건드리면 돌아갈 곳이 없다 1편에서 Bronze 레이어의 원칙을 정했다. 소스 시스템에서 가져온 데이터를 변환 없이 저장한다. 타입 캐스팅도 안 하고, 컬럼명도 안 바꾼다.\n원칙은 간단한데 실제로 지키기가 어렵다. \u0026ldquo;날짜 컬럼 타입이 문자열인데 DATE로 바꿔서 넣으면 안 되나?\u0026rdquo; 같은 유혹이 생긴다. 안 된다. Bronze에서 타입을 바꾸면 원본 복원이 불가능해진다. 소스 시스템에서 \u0026quot;2026-02-30\u0026quot; 같은 잘못된 날짜가 넘어왔을 때, DATE로 캐스팅하면 에러가 나거나 NULL로 바뀐다. 원본이 뭐였는지 알 수 없게 된다.\nBronze는 보험이다. Silver 변환 로직에 버그가 있어도, 소스 시스템이 갑자기 스키마를 바꿔도, Bronze에서 다시 시작할 수 있다. 이 보험을 포기하면 문제가 생길 때마다 소스 시스템에서 데이터를 다시 끌어와야 한다. 소스 시스템 담당자가 협조적이라는 보장은 없다.\nFull Load와 Incremental Load Bronze에 데이터를 넣는 방법은 크게 두 가지다.\nFull Load 는 소스 테이블 전체를 매번 가져와서 덮어쓴다. 단순하다. 소스에 있는 그대로가 Bronze에 있으니까 정합성 고민이 없다. 대신 데이터가 커지면 비용이 늘어난다. 주문 테이블이 1억 건인데 하루에 신규 주문이 1만 건이라면, 나머지 9,999만 건은 어제와 똑같은 데이터를 매번 다시 가져오는 셈이다.\nIncremental Load 는 마지막 적재 이후에 변경된 데이터만 가져온다. 효율적이다. 1만 건만 가져오면 된다. 대신 복잡하다. \u0026ldquo;마지막 적재 이후\u0026quot;를 어떻게 판단할 건지, 삭제된 데이터는 어떻게 감지할 건지 정해야 한다.\n어떤 걸 쓸지는 테이블 특성에 따라 다르다.\n구분 Full Load Incremental Load 구현 난이도 낮음 높음 네트워크/비용 데이터 크기에 비례 변경분에 비례 삭제 감지 자동 (전체를 덮어쓰니까) 별도 처리 필요 적합한 대상 코드 테이블, 소규모 마스터 대용량 트랜잭션 실무에서는 섞어 쓴다. 코드 테이블이나 상품 마스터처럼 건수가 적은 테이블은 Full Load로 단순하게 가져간다. 주문, 로그, 이벤트처럼 건수가 많은 테이블은 Incremental Load로 변경분만 가져간다.\n증분의 기준을 잡는 법 Incremental Load에서 가장 중요한 건 \u0026ldquo;무엇이 변경되었는가\u0026quot;를 판단하는 기준이다. 흔히 쓰는 방법이 세 가지 있다.\n타임스탬프 기반. 소스 테이블에 updated_at 같은 수정일시 컬럼이 있으면 가장 간단하다. 마지막 적재 시점 이후의 행만 가져온다. 조건이 하나 있다. 소스 시스템이 수정일시를 정직하게 갱신해야 한다. 데이터를 UPDATE하면서 updated_at을 안 바꾸는 시스템이 의외로 많다.\n자동 증가 키 기반. order_id처럼 단조 증가하는 PK가 있으면 마지막으로 가져온 ID 이후의 행만 가져온다. INSERT는 잡히지만 UPDATE는 못 잡는다. 주문번호가 한 번 발행되면 바뀌지 않는 로그성 테이블에 적합하다.\nCDC(Change Data Capture). 소스 데이터베이스의 변경 로그를 직접 읽는다. Debezium 같은 도구가 MySQL이나 PostgreSQL의 WAL(Write-Ahead Log)을 캡처해서 INSERT, UPDATE, DELETE를 전부 잡아낸다. 가장 정확하지만 인프라 구성이 필요하다.\n타임스탬프 기반: WHERE updated_at \u0026gt; \u0026#39;마지막 적재 시점\u0026#39; 자동 증가 키: WHERE order_id \u0026gt; 마지막_적재_ID CDC: 데이터베이스 변경 로그 캡처 DuckDB로 두 방식을 직접 비교한다 1편에서 세팅한 환경을 이어서 쓴다.\nimport duckdb conn = duckdb.connect(\u0026#39;warehouse.duckdb\u0026#39;) Full Load 시뮬레이션 Full Load는 간단하다. 기존 데이터를 지우고 전체를 다시 넣는다.\n# 소스 데이터가 변경된 상황을 시뮬레이션 # 실제로는 소스 시스템에서 SELECT * 로 전체를 가져온다 conn.execute(\u0026#34;\u0026#34;\u0026#34; -- Full Load: 통째로 교체 CREATE OR REPLACE TABLE bronze.orders AS SELECT * FROM read_csv_auto( \u0026#39;https://raw.githubusercontent.com/dbt-labs/jaffle_shop/main/seeds/raw_orders.csv\u0026#39; ); \u0026#34;\u0026#34;\u0026#34;) print(\u0026#34;Full Load 완료:\u0026#34;, conn.execute(\u0026#34;SELECT count(*) FROM bronze.orders\u0026#34;).fetchone()[0], \u0026#34;건\u0026#34;) CREATE OR REPLACE TABLE이 핵심이다. 매번 테이블을 새로 만든다. 이전 데이터는 사라지고 소스의 현재 상태가 그대로 들어온다.\nIncremental Load 시뮬레이션 Incremental Load는 한 단계가 더 있다. 마지막으로 가져온 지점을 기억해야 한다.\n# 워터마크 테이블: 마지막 적재 지점을 기록 conn.execute(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE IF NOT EXISTS bronze.watermarks ( table_name VARCHAR PRIMARY KEY, last_loaded_id INTEGER, last_loaded_at TIMESTAMP DEFAULT current_timestamp ); \u0026#34;\u0026#34;\u0026#34;) # 현재 워터마크 확인 watermark = conn.execute(\u0026#34;\u0026#34;\u0026#34; SELECT COALESCE(last_loaded_id, 0) FROM bronze.watermarks WHERE table_name = \u0026#39;orders\u0026#39; \u0026#34;\u0026#34;\u0026#34;).fetchone() last_id = watermark[0] if watermark else 0 print(f\u0026#34;마지막 적재 ID: {last_id}\u0026#34;) # 증분 적재: last_id 이후 데이터만 가져온다 conn.execute(f\u0026#34;\u0026#34;\u0026#34; INSERT INTO bronze.orders SELECT * FROM read_csv_auto( \u0026#39;https://raw.githubusercontent.com/dbt-labs/jaffle_shop/main/seeds/raw_orders.csv\u0026#39; ) WHERE id \u0026gt; {last_id}; \u0026#34;\u0026#34;\u0026#34;) # 워터마크 갱신 conn.execute(\u0026#34;\u0026#34;\u0026#34; INSERT OR REPLACE INTO bronze.watermarks (table_name, last_loaded_id, last_loaded_at) SELECT \u0026#39;orders\u0026#39;, MAX(id), current_timestamp FROM bronze.orders; \u0026#34;\u0026#34;\u0026#34;) print(\u0026#34;Incremental Load 완료\u0026#34;) watermarks 테이블이 증분 적재의 핵심이다. 어디까지 가져왔는지를 기록해두고, 다음 적재 때 그 이후만 가져온다. 이 패턴을 하이 워터마크(High Watermark) 라고 부른다.\n메타데이터 컬럼을 붙인다 Bronze에 원본 데이터만 넣으면 나중에 답이 안 나오는 질문이 생긴다. \u0026ldquo;이 데이터가 언제 적재된 건가?\u0026rdquo; \u0026ldquo;어느 소스에서 온 건가?\u0026rdquo;\n원본 컬럼은 그대로 두고, 메타데이터 컬럼을 추가한다.\nconn.execute(\u0026#34;\u0026#34;\u0026#34; CREATE OR REPLACE TABLE bronze.orders_with_meta AS SELECT *, current_timestamp AS _loaded_at, \u0026#39;jaffle_shop\u0026#39; AS _source_system, \u0026#39;full\u0026#39; AS _load_type FROM read_csv_auto( \u0026#39;https://raw.githubusercontent.com/dbt-labs/jaffle_shop/main/seeds/raw_orders.csv\u0026#39; ); \u0026#34;\u0026#34;\u0026#34;) conn.execute(\u0026#34;SELECT * FROM bronze.orders_with_meta LIMIT 3\u0026#34;).fetchdf() _loaded_at, _source_system, _load_type. 언더스코어로 시작하는 이유는 원본 컬럼과 구분하기 위해서다. 원본에 loaded_at이라는 컬럼이 있을 수도 있으니까.\n이 메타데이터가 있으면 Silver 변환에서 문제가 생겼을 때 \u0026ldquo;언제 적재한 데이터까지는 정상이고, 이후부터 이상하다\u0026quot;는 식으로 범위를 좁힐 수 있다.\n적재 패턴 정리 Bronze 적재 패턴을 정리하면 이렇다.\n패턴 적용 대상 구현 Full Load (덮어쓰기) 코드 테이블, 소규모 마스터 CREATE OR REPLACE TABLE Full Load (스냅샷) 일별 현황 보관이 필요한 경우 파티션 키로 적재일 사용 Incremental (타임스탬프) updated_at이 있는 테이블 WHERE updated_at \u0026gt; 워터마크 Incremental (자동 증가 키) 로그, 이벤트, 주문 WHERE id \u0026gt; 워터마크 CDC 삭제 감지가 필요한 경우 Debezium + Kafka Full Load 중에 스냅샷 방식이 하나 더 있다. 덮어쓰기가 아니라 적재일 기준으로 매일의 전체 상태를 따로 저장하는 방식이다. 상품 마스터의 어제 상태와 오늘 상태를 비교하고 싶을 때 쓴다. 스토리지를 많이 먹지만, 1편에서 얘기했듯 클라우드 환경에서 스토리지 비용은 무시할 수 있는 수준이다.\n다음 글에서는 Silver 레이어를 다룬다. Bronze에 쌓아둔 원본 데이터를 정제하고 표준화하는 과정이다. dbt를 본격적으로 쓰기 시작한다.\nGoogle Colab에서 실습하기\r","permalink":"https://biz-agentic-ai.github.io/guides/etl-design/002-bronze-layer/","summary":"Bronze에 데이터를 넣는 방법은 두 가지다. 전체를 덮어쓰거나, 바뀐 것만 가져오거나. 어떤 방식을 고르느냐에 따라 파이프라인의 복잡도가 완전히 달라진다.","title":"2. Bronze 레이어 - 원본을 있는 그대로 쌓는다"},{"content":"고객 테이블에 주민번호와 사업자번호가 같이 있다 기간계 시스템에서 흔히 보는 구조다. 고객 테이블 하나에 개인고객 속성(주민번호, 생년월일)과 법인고객 속성(사업자번호, 대표자명)이 섞여 있다. 개인고객이면 사업자번호가 NULL이고, 법인고객이면 주민번호가 NULL이다. 고객유형코드 컬럼 하나로 구분한다.\n데이터가 적을 때는 별 문제가 안 된다. 고객이 수천만 건이 되면 이야기가 달라진다. 개인고객에만 필요한 컬럼이 법인고객 행에도 자리를 차지하고, 법인고객에만 필요한 컬럼이 개인고객 행에서 NULL로 비어 있다. 컬럼이 늘어날수록 테이블이 와이드해지고 의미가 흐려진다. \u0026ldquo;이 컬럼이 어떤 고객 유형에 해당하는 건지\u0026quot;를 DDL만 보고 알기 어렵다.\n수퍼-서브 타입은 이 문제를 논리 모델 단계에서 정리하는 방법이다.\n공통 속성과 고유 속성을 분리한다 수퍼-서브 타입의 원리는 간단하다. 공통 속성은 수퍼 타입(고객)에 두고, 유형별 고유 속성은 서브 타입(개인고객, 법인고객)에 둔다.\n[고객] ← 수퍼 타입: 고객ID, 고객명, 연락처 ├─ [개인고객] ← 서브 타입: 주민번호, 생년월일 └─ [법인고객] ← 서브 타입: 사업자번호, 대표자명 고객ID 하나로 수퍼 타입과 서브 타입이 연결된다. 개인고객 테이블에는 개인고객에만 해당하는 속성만 들어간다. NULL 투성이의 와이드 테이블이 사라진다.\n서브 타입을 나누는 또 다른 이유가 있다. 서브 타입별로 다른 엔터티와 독립적으로 관계를 맺을 수 있다. 법인고객만 여신한도와 관계를 가진다든지, 개인고객만 멤버십 등급과 관계를 가진다든지. 수퍼 타입 하나에 모든 관계를 매달면 관계의 의미가 모호해지는데, 서브 타입으로 나누면 \u0026ldquo;이 관계가 어떤 유형에 해당하는지\u0026quot;가 모델에서 바로 읽힌다.\n배타적인가, 중복 가능한가 서브 타입을 설계할 때 반드시 먼저 따지는 게 있다. 하나의 인스턴스가 서브 타입 중 정확히 하나에만 속하는지(Exclusive), 여러 서브 타입에 동시에 속할 수 있는지(Inclusive)다.\nExclusive 가 압도적으로 많다. 고객은 개인 아니면 법인이다. 계좌는 보통예금, 적금, 정기예금 중 하나다. 상품은 실물이거나 디지털이다. 구분코드 하나로 분류가 끝난다.\nInclusive 는 드물지만 빠뜨리면 나중에 큰 수정이 필요하다. 서비스 상품 같은 경우가 해당된다. 하나의 상품이 B2B 대상이면서 동시에 B2C 대상일 수 있다. 직원 역할도 마찬가지다. 한 사람이 영업과 기술지원을 겸하는 경우, \u0026ldquo;직원역할\u0026rdquo; 서브 타입이 Inclusive가 된다.\n설계 초기에 \u0026ldquo;이 분류가 정말 배타적인가\u0026quot;를 한 번 더 따져야 한다. Exclusive라고 전제하고 모델을 짠 뒤에 겹치는 케이스가 나오면 구분코드 체계부터 관계 구조까지 뜯어고쳐야 한다.\n물리 모델로 넘어갈 때의 선택지 논리 모델에서 수퍼-서브 타입은 깔끔하다. 물리 모델로 전환할 때 선택이 갈린다.\n통합 테이블. 수퍼 타입과 서브 타입을 하나의 테이블로 합친다. 처음에 문제라고 했던 그 와이드 테이블이 되지만, 조인이 없어서 쿼리가 단순하다. 서브 타입별 고유 속성이 적으면 실용적인 선택이다.\n개별 테이블. 수퍼 타입 테이블과 서브 타입 테이블을 각각 만든다. NULL이 없고 구조가 명확하지만, 고객 정보를 온전히 보려면 수퍼 타입과 서브 타입을 조인해야 한다.\n서브 타입만. 수퍼 타입 테이블 없이 개인고객 테이블, 법인고객 테이블만 만든다. 공통 속성을 각 테이블에 중복으로 가진다. 서브 타입별로 완전히 독립적인 분석을 하는 경우에 맞지만, 고객 전체를 보려면 UNION이 필요하다.\n정답은 없다. 서브 타입 수, 고유 속성의 양, 쿼리 패턴에 따라 달라진다.\nDW 차원 설계와의 연결 DW에서는 이 선택이 차원(Dimension) 설계와 직결된다. 2편에서 다뤘던 \u0026ldquo;접근 경로\u0026rdquo; 관점이 판단 기준이 된다.\n고객 차원을 설계한다고 하자. 개인고객과 법인고객을 별도 차원으로 나누면 팩트 테이블에 FK가 늘어나고, 분석할 때 어떤 차원을 조인할지 매번 선택해야 한다. 통합 차원으로 만들면 NULL이 많은 와이드 테이블이 되지만, 1편에서 다뤘던 것처럼 클라우드 Columnar Storage에서는 NULL 컬럼의 스캔 비용이 거의 없다.\n판단 기준은 분석 패턴이다. 개인고객 매출은 연령대와 지역으로 보고, 법인고객 매출은 산업군과 매출 규모로 본다면 차원 속성 자체가 다르니 나누는 게 낫다. 고객 전체를 하나의 축으로 놓고 보는 분석이 대부분이면 통합이 편하다.\n실무에서 많이 보는 절충안은 통합 차원을 기본으로 두되, 서브 타입별 분석이 빈번한 경우 별도 뷰나 마트를 추가하는 방식이다. 클라우드 환경에서 스토리지 비용이 낮으니 중복 저장의 부담이 작다.\n다음 글에서는 DW 설계의 두 가지 큰 흐름인 Inmon 방식과 Kimball 방식을 비교한다. 1편에서 간략히 언급했던 내용을 더 구체적으로 들어간다.\n","permalink":"https://biz-agentic-ai.github.io/guides/dw-modeling/004-super-sub-type/","summary":"수퍼-서브 타입은 논리 모델에서 비즈니스 분류를 명확하게 만든다. 물리 모델로 넘어갈 때 세 가지 선택지가 생기고, DW에서는 그 선택이 차원 설계 전체를 바꾼다.","title":"4. 수퍼-서브 타입 - 고객이 개인이면서 법인일 수 있는가"},{"content":"툴을 바꾸면 모델이 다르게 보인다 DW 프로젝트에 투입되면 보통 기존 모델부터 받아서 훑는다. 이때 한 가지 확인하지 않으면 나중에 고생하는 게 있다. 모델을 그린 툴이 뭔지, 그 툴의 표기법이 뭔지.\nERwin으로 설계한 모델을 DA#으로 열면 관계선 해석이 달라진다. 관계선의 점선이 한쪽에서는 \u0026ldquo;비식별 관계\u0026quot;인데 다른 쪽에서는 \u0026ldquo;선택적 참여\u0026quot;다. 둘 다 정상이다. 표기법이 다를 뿐이다. 문제는 이걸 모른 채 모델 리뷰를 하면 같은 ERD를 놓고 서로 다른 이야기를 하게 된다는 것이다.\nERD는 모델러, 개발자, 현업이 공통으로 읽는 언어다. 그 언어에 방언이 여러 개 있다는 걸 모르면 의사소통이 안 된다.\n같은 까마귀발인데 해석이 갈린다 ERD 표기법 중 가장 널리 쓰이는 건 까마귀발(Crow\u0026rsquo;s Foot) 계열이다. 관계선 끝에 대시(1), 원(0), 까마귀발(N) 기호를 조합해서 카디널리티를 표현한다. 여기까지는 공통이다.\n문제는 까마귀발 안에 두 계열이 있다는 점이다.\nIE(Information Engineering) 방식 은 ERwin, PowerDesigner 등에서 기본으로 쓴다. 식별 관계를 실선, 비식별 관계를 점선으로 구분한다. 식별 관계란 상위 엔터티의 PK가 하위 엔터티 PK의 일부로 포함되는 것이다.\nBarker 방식 은 Oracle 계열 툴과 DA#에서 쓴다. 겉보기엔 같은 까마귀발인데 기호의 의미가 다르다.\n기호 IE 방식 Barker 방식 점선 비식별 관계 선택적 참여 (0 or 1) 실선 식별 관계 필수 참여 (정확히 1) 대시 정확히 1 식별 관계 점선의 의미가 완전히 다르다. IE에서 점선은 \u0026ldquo;상위 PK가 하위 PK에 포함되지 않는다\u0026quot;는 구조적 정보다. Barker에서 점선은 \u0026ldquo;참여가 선택적이다\u0026quot;라는 비즈니스 규칙이다. 같은 기호가 다른 층위의 정보를 담고 있다.\nDA#으로 설계한 모델을 ERwin으로 가져와서 리뷰하면 이 차이를 모르는 사람이 점선을 전부 비식별 관계로 읽어버린다. 설계 의도가 왜곡되는 거다.\n까마귀발을 안 쓰는 표기법도 있다 IDEF1X라는 표기법은 까마귀발 대신 원으로 카디널리티를 나타낸다. 원이 없으면 정확히 1, 빈 원은 0 또는 1, 속이 찬 원은 0 또는 N이다. 식별/비식별은 IE와 동일하게 실선/점선으로 구분한다.\n변형도 있다. 모든 원을 속이 찬 원으로 통일하면서 Z(0 or 1), P(1 or N) 같은 문자를 붙이는 방식이다. ERwin에서 IDEF1X 모드로 전환하면 이걸 쓸 수 있다.\n까마귀발 계열과 IDEF1X는 카디널리티를 표현하는 기호 체계 자체가 다르다. 까마귀발 계열 내에서의 혼동(IE vs Barker)이 같은 기호를 다르게 읽는 문제라면, IDEF1X와의 차이는 기호 자체를 모르면 읽지 못하는 문제다. 차원이 다르다.\n프로젝트에서 부딪히는 현실 표기법을 외우라는 게 아니다. 프로젝트에서 실제로 생기는 문제를 미리 아는 게 중요하다.\n모델링 툴을 바꿀 때 표기법 전환이 자동으로 되기는 하지만 완벽하지 않다. 세부 표현이 달라지거나, 뒤에서 다룰 수퍼-서브 타입처럼 구조 자체가 바뀌는 경우도 있다. 같은 IE 방식이라도 툴에 따라 대시 2개를 하나로 표현하는 것 같은 미세한 차이가 있다.\n프로젝트 시작할 때 세 가지를 맞추면 된다.\n어떤 표기법을 쓸 건지 정한다 팀 전원이 해당 표기법의 기호 의미를 안다 툴의 도움말에서 표기법 상세를 한 번은 확인한다 이게 안 된 프로젝트에서는 모델 리뷰가 표기법 논쟁으로 빠진다. 설계를 논의해야 할 시간에 \u0026ldquo;이 선이 뭔 뜻이냐\u0026quot;를 따지고 있으면 비용이다.\n코드로 그리는 ERD 클라우드 DW 환경에서는 ERD를 GUI 툴로 그리지 않는 팀도 많다. dbt에서 SQL 기반으로 모델을 정의하고, Mermaid나 DBML 같은 텍스트 기반 다이어그램으로 관계를 표현한다. 코드 리뷰처럼 모델 변경 이력을 추적할 수 있다는 게 장점이다.\n도구가 바뀌어도 표현해야 하는 건 같다. 카디널리티, 식별/비식별 관계, 필수/선택 참여. 이 개념을 이해하지 못하면 GUI 툴이든 텍스트 기반이든 모델을 제대로 읽을 수 없다.\n다음 글에서는 수퍼-서브 타입을 다룬다. 고객을 개인고객과 법인고객으로 나누는 구조인데, 논리 모델에서 물리 모델로 넘어갈 때 선택지가 갈리고 DW에서는 그 선택이 차원 설계와 직결된다.\n","permalink":"https://biz-agentic-ai.github.io/guides/dw-modeling/003-erd-notation/","summary":"같은 까마귀발인데 왜 해석이 다를까. 점선 하나가 툴마다 다른 뜻을 가진다. 프로젝트에서 모델을 공통 언어로 쓰려면 쓰는 도구의 표기법부터 맞춰야 한다.","title":"3. ERD 표기법 - 같은 그림, 다른 해석"},{"content":"\u0026ldquo;이 Unknown은 누가 넣은 건가요?\u0026rdquo; DW 모델 리뷰 자리에서 꼭 나오는 질문이다.\n상품 마스터 테이블을 열어보면 \u0026ldquo;Unknown\u0026quot;이라는 이름의 데이터가 들어 있다. 사원 테이블에도 있다. 기간계 시스템을 해온 사람이라면 당연히 의아하다. 마스터 테이블에 더미 데이터라니.\n비슷한 질문이 뒤따른다. \u0026ldquo;주문실적 테이블에 시점담당사원이라는 컬럼은 뭔가요? 기간계 주문 테이블에는 없던 건데.\u0026rdquo; DW 모델을 처음 접한 사람에게는 이것도 낯설다.\n두 모델의 차이를 키워드로 설명하는 자료는 많다. 비정규화, 스타스키마, 스노우플레이크. 검색하면 바로 나온다. 문제는 키워드만으로 \u0026ldquo;왜 이렇게 설계하는가\u0026quot;가 설명이 안 된다는 점이다. 목적부터 짚어야 한다.\n기간계 모델은 트랜잭션을 지킨다 OLTP 데이터 모델의 목표는 명확하다. 빈번한 입력과 수정 과정에서 정합성을 깨뜨리지 않는 것.\n이 목표가 모델의 생김새를 결정한다. 엔터티 사이의 관계가 엄격하다. 부서가 없으면 사원을 등록할 수 없고, 상품이 없으면 주문이 발생할 수 없다. 고객이 없는 주문도 존재하지 않는다. 모든 관계에는 선행 조건이 있고, 데이터가 발생하는 그 시점에 조건이 충족되어야 한다.\n이걸 보장하기 위해 정규화를 한다. 중복을 줄이면 수정할 곳이 한 군데로 줄고, 정합성이 깨질 여지가 작아진다. 최상위 마스터(코드 테이블 같은)부터 순서대로 등록하고, 그 위에 트랜잭션 데이터를 쌓는다. 순서가 틀어지면 안 된다.\n비유하면 이렇다. 할아버지가 있어야 아버지가 있고, 아버지가 있어야 아들이 있다. 존재 관계다. 사람이 있어야 사람의 행동이 기록된다. 행위 관계다. OLTP 모델은 이런 관계의 제약 조건을 빠짐없이 반영하는 데 집중한다.\nDW 모델은 접근 경로를 설계한다 DW 데이터 모델은 다른 문제를 푼다. 모든 데이터를 빠짐없이 적재하고, 분석 대상에 접근하는 경로를 만드는 것이다.\n\u0026ldquo;접근 경로\u0026quot;가 핵심이다. 주문실적이라는 분석 대상이 있다고 하자. 사원 기준으로도, 상품 기준으로도, 고객 기준으로도 들어갈 수 있어야 한다. 어느 경로로 가든 같은 결과가 나와야 하고, 성능도 비슷해야 한다. 스타스키마가 이 구조를 가장 직관적으로 표현한다.\n[사원] | [상품] — 주문실적 — [고객] | [직업] 주문실적을 중심에 놓고, 접근 경로가 되는 차원 테이블이 주변을 둘러싸는 형태다.\nOLTP 경험이 많은 사람이 이 모델을 보면 \u0026ldquo;비정규화한 OLTP\u0026quot;라고 오해하기 쉽다. ERD라는 도구가 같으니까 결과물도 같은 종류일 거라고 생각한다. 도구가 같을 뿐이다. 설계의 출발점이 다르다.\n시점 데이터라는 낯선 개념 OLTP 주문 테이블에는 \u0026ldquo;담당사원\u0026rdquo; 컬럼이 있다. 현재 담당사원을 가리킨다. DW의 주문실적 테이블에는 시점담당사원 이 있다. 주문이 발생한 바로 그 시점의 담당사원이다.\n왜 이런 게 필요한가. 상품 담당사원이 올해 A에서 B로 바뀌었다고 하자. OLTP에서는 현재 담당이 B다. 그걸로 끝이다. DW에서는 상황이 다르다. \u0026ldquo;작년 실적은 A 기준으로, 올해 실적은 B 기준으로 보고 싶다\u0026quot;는 요구가 자연스럽게 나온다. 상품담당사원이력이나 고객직업이력 같은 이력 데이터를 활용해서, 주문실적 적재 시점에 시점 데이터를 함께 만들어낸다.\nOLTP에서 퇴사한 사원은 마스터에서 비활성화하면 그만이다. DW에서는 과거 시점에만 존재했던 사원도, 더 이상 유효하지 않은 직업 코드도 마스터 테이블에 전부 남겨야 한다. 과거 분석에 필요한 데이터가 빠지면 안 되니까.\nUnknown이 존재하는 이유 DW 프로젝트에서 흔한 상황이 하나 있다. 과거 10년치 주문실적을 분석하려는데, 상품 마스터 관리가 부실해서 최근 상품만 남아 있다. 주문실적에는 상품ID가 찍혀 있는데 상품 테이블에는 해당 ID가 없다.\nOLTP였으면 이런 일 자체가 안 일어난다. 상품이 없으면 주문이 생길 수 없도록 설계했으니까. DW는 입장이 다르다. 이미 발생한 과거 데이터를 있는 그대로 적재해야 한다.\n이때 선택지가 몇 가지 있다.\n주문실적의 상품ID를 Unknown에 해당하는 ID로 바꾸거나 분석용 상품ID 컬럼을 하나 더 두어 이중 관리하거나 매핑 안 되는 상품ID를 적재 시점에 상품 마스터에 먼저 추가하고, 나머지 속성은 NULL이나 대체값으로 채우거나 어떤 방식이든 한 가지는 공통이다. 상품 마스터에 Unknown 이라는 기준 데이터를 미리 넣어 둔다는 것. 해당 상품의 담당사원도 알 수 없으니 사원 테이블에도 Unknown을 넣는다. 엔터티 간 관계를 형식적으로 충족시키되, 데이터가 발생한 시점이 아니라 적재하는 시점에 인위적으로 맞추는 방식이다.\nOLTP 모델러가 보면 불편할 수 있다. 인위적인 더미 데이터로 관계를 맞추다니. DW의 목적을 생각하면 합리적인 판단이다. 분석 대상 데이터를 빠뜨리지 않으면서, 어떤 접근 경로로 들어가든 일관된 구조가 유지되어야 하니까.\n관계를 맞추는 시점이 다르다 정리하면 이렇다.\nOLTP 는 데이터가 발생하는 시점에 관계 조건을 충족시킨다. 부서 없이 사원을 등록할 수 없고, 고객 없이 주문을 넣을 수 없다. 관계를 어기면 데이터 자체가 들어가지 않는다.\nDW 는 데이터를 적재하는 시점에 관계를 맞춘다. 원본에 누락이 있으면 Unknown으로 채운다. 과거 시점 데이터가 필요하면 이력에서 끌어와서 만든다. 적재 과정에서 일정한 개입을 통해 관계를 충족시키는 것이다.\nOLTP DW 목적 트랜잭션 처리, 정합성 보장 분석 데이터 적재, 접근 경로 설계 관계 충족 시점 데이터 발생 시점 데이터 적재 시점 누락 데이터 허용하지 않음 Unknown으로 처리 이력 관리 현재 상태 중심 시점 데이터 생성 설계 방향 정규화 (중복 최소화) 접근 경로 중심 (분석 편의) 이 차이를 이해하고 나면 DW 모델에서 \u0026ldquo;왜 이렇게 했지?\u0026ldquo;라는 의문이 상당 부분 풀린다.\n클라우드 시대에도 같은 이야기 이전 글에서 클라우드 DW의 물리적 제약 변화를 다뤘다. 스토리지가 싸졌고, 컬럼나 스토리지 덕에 조인 패턴이 달라졌고, ELT 패러다임으로 전환됐다.\n물리적 제약은 바뀌었지만 OLTP와 DW의 목적 차이는 여전하다. BigQuery를 쓰든 Synapse를 쓰든, 분석 데이터에 대한 접근 경로를 설계해야 하는 건 마찬가지다. Unknown 레코드가 필요한 상황도, 시점 데이터를 관리해야 하는 요건도 인프라가 바뀐다고 없어지지 않는다.\n달라진 게 있다면 이력 관리를 더 적극적으로 할 수 있게 됐다는 것 정도다. 스토리지 부담이 줄어서 SCD Type 2 방식으로 차원 이력을 쌓아도 부담이 덜하다. SCD 유형별 설계 방식은 시리즈 뒤쪽에서 구체적으로 다룬다.\n다음 글에서는 모델을 표현하는 도구, ERD 표기법의 차이를 짚어본다. 같은 관계를 그려놓아도 Crow\u0026rsquo;s Foot이냐 IDEF1X이냐에 따라 해석이 달라진다. 도구의 언어를 모르면 같은 모델을 보고도 다른 이야기를 하게 된다.\n","permalink":"https://biz-agentic-ai.github.io/guides/dw-modeling/002-oltp-vs-dw-model/","summary":"ERD가 같아 보여도 설계 철학은 완전히 다르다. OLTP는 트랜잭션 정합성, DW는 분석 접근 경로. 그 차이가 Unknown 레코드와 시점 데이터 같은 낯선 것들을 만든다.","title":"2. OLTP vs DW 모델 - 목적이 다르면 설계도 다르다"},{"content":"왜 갑자기 외부 시스템 연동을 이야기하나 3편까지는 하나의 관점으로만 글을 썼다. NL2SQL 정확도. \u0026ldquo;LLM에게 비즈니스 맥락을 얼마나 잘 주입할 수 있느냐\u0026quot;가 모든 의사결정의 기준이었다.\n여기서부터 관점이 하나 더 추가된다. 플랫폼이다.\nDataNexus가 한 고객사 안에서만 돌아가는 NL2SQL 도구로 끝난다면, 외부 시스템 연동은 필요 없다. DozerDB 그래프가 잘 돌아가면 그만이다. 문제는 1편에서 이미 그보다 큰 그림을 그려놨다는 거다 — 그룹사별 멀티테넌시, Data Moat, 시간축 지식그래프. 이 단어들은 전부 \u0026ldquo;DataNexus가 단일 시스템이 아니라 여러 조직이 온톨로지를 교환하는 플랫폼이 되어야 한다\u0026quot;는 전제 위에 있다.\n유통 그룹이 백화점·마트·온라인몰을 갖고 있는데, 관계사마다 \u0026ldquo;매출\u0026quot;의 정의가 다르다. 이걸 통합하려면 각 관계사의 온톨로지를 공통 포맷으로 내보내서 매핑해야 한다. DataNexus만의 독자 포맷으로는 이 작업이 안 된다.\nSKOS 호환 레이어가 NL2SQL 정확도를 직접 올려주진 않는다. 대신 다른 방식으로 도움이 된다.\n금융 도메인의 FIBO나 유통의 GPC 같은 산업 표준 온톨로지를 가져오면, 밑바닥부터 용어를 정의하는 시간이 줄어든다. 온톨로지 구축이 빨라지면 NL2SQL 엔진에 맥락이 주입되는 시점이 앞당겨진다. 1편에서 \u0026ldquo;범용 모델의 일반화 속도를 DataNexus의 데이터 축적 속도가 앞서야 한다\u0026quot;고 썼는데, 축적 속도를 올리는 방법 중 하나가 표준을 가져다 쓰는 것이다. 고객사가 이미 Collibra나 Alation을 쓰고 있는 경우, DataNexus 온톨로지를 표준 포맷으로 내보낼 수 없으면 도입 자체가 막힌다. 아무리 NL2SQL 정확도가 높아도 기존 인프라와 공존 못 하면 현장에서 안 쓴다. 이건 유통사 프로젝트에서 겪은 교훈이다 — 기술보다 현장 적합성이 도입을 결정한다. 4편은 NL2SQL 엔진의 내부 성능 이야기가 아니다. DataNexus가 플랫폼으로 기능하기 위한 인터페이스 설계 이야기다. 관점이 다르니까 풀어야 할 문제도 다르다.\n외부 시스템과 연동이 안되었다. 이전 글에서 DataHub + DozerDB 이중 구조로 내부 온톨로지 문제를 풀었다. 내부 시스템에서만 쓰기에는 충분했다.\n문제는 외부 시스템과의 연동이었다. 금융 도메인을 탐색하다가 FIBO(Financial Industry Business Ontology)를 발견했는데, 금융업계 표준 용어 체계로 \u0026ldquo;Financial Product\u0026rdquo;, \u0026ldquo;Loan\u0026rdquo;, \u0026ldquo;Interest Rate\u0026rdquo; 같은 개념이 계층으로 정리돼 있다. 유통 쪽도 마찬가지다. GS1의 GPC(Global Product Classification)에는 \u0026ldquo;의류 → 여성복 → 원피스\u0026quot;처럼 상품 분류 체계가 표준으로 잡혀 있다. 의료엔 SNOMED CT, 제조엔 ISA-95. 도메인마다 수천 개 용어가 이미 정리돼 있는데, 이걸 가져다 쓸 수 있으면 온톨로지를 밑바닥부터 만들 필요가 없다.\nFIBO 파일을 열어봤다. OWL 포맷이었다. DozerDB 그래프에 넣으려니 구조 자체가 안 맞았다. 반대 방향도 마찬가지 — DataNexus 온톨로지를 고객사 기존 시스템(Collibra, TopBraid 같은)에 내보내고 싶어도 표준 포맷이 없으니 방법이 없었다. 내부에서는 잘 돌아가는데 밖으로 꺼내는 순간 무용지물이 되는 상황.\n외부 호환이 안 되면 생기는 문제가 한두 개가 아니다. 대기업은 이미 Collibra나 Alation 같은 메타데이터 관리 툴을 쓰고 있는 경우가 많다. DataNexus를 도입한다고 기존 용어 체계를 버리진 않는다. 표준 포맷으로 내보낼 수 있으면 공존이 가능한데, 못하면 용어 수백 개를 수작업으로 옮겨야 한다. 그것만으로 몇 달이 날아간다.\n유통 그룹처럼 백화점·마트·온라인몰이 각각 \u0026ldquo;매출\u0026quot;을 다르게 정의하는 경우, 그룹 차원에서 용어를 통합하거나 최소한 매핑하려면 공통 포맷이 있어야 한다. 없으면 관계사마다 따로 논다. 금융권은 감독 기관에 데이터 계보(lineage)나 용어 정의를 보고해야 하는 규제 요건도 있다. 거기에 벤더 종속(vendor lock-in) 문제까지. DataNexus를 쓰다가 다른 플랫폼으로 바꿔야 할 수도 있는데, 표준 포맷으로 Export가 되면 옮길 수 있지만 안 되면 갇힌다. 도입을 결정하는 자리에서 이게 꽤 크게 작용한다.\n내부에서만 통하는 언어로는 외부와 대화할 수 없다.\n같은 그래프인데 언어가 다르다 DozerDB는 LPG(Labeled Property Graph) 방식을 쓴다.\n노드(동그라미)에 이름과 속성을 붙인다: 순매출 {definition: \u0026quot;총매출-반품-에누리\u0026quot;} 노드 사이에 화살표를 긋고, 그 화살표에도 속성을 단다: -[MANUFACTURES {since: \u0026quot;2024-01-01\u0026quot;}]-\u0026gt; 핵심은 화살표 자체에 \u0026ldquo;언제부터\u0026rdquo;, \u0026ldquo;신뢰도 얼마\u0026rdquo; 같은 정보를 달 수 있다는 점이다. 이전 글에서 MANUFACTURES, STOCKS 관계를 만들 때 이걸 활용했다.\nSKOS를 포함한 웹 표준들은 완전히 다른 체계를 쓴다. RDF(Resource Description Framework) — 모든 정보를 세 단어짜리 문장으로 쪼갠다.\n순매출 → broader → 매출 (순매출의 상위 개념은 매출이다) 순매출 → prefLabel → \u0026quot;순매출\u0026quot;@ko (한국어 이름은 \u0026ldquo;순매출\u0026quot;이다) 주술목(주어-서술어-목적어), 이 세 단어가 하나의 단위다. 트리플(triple) 이라고 부른다.\n여기서 갈린다. LPG는 관계에 속성을 자유롭게 붙일 수 있지만, RDF는 트리플이 원자 단위라서 관계 자체에 속성을 직접 달 수 없다. 대신 URI 기반이라 전 세계 어디서든 같은 개념을 같은 주소로 가리킬 수 있다. 시스템 간 데이터 교환에는 RDF가 압도적이다.\n내부 표현력의 LPG, 외부 호환성의 RDF. DataNexus에는 둘 다 필요했다.\nOWL은 과하고, RDFS는 부족하고 RDF 세계에도 표준이 여러 개다.\nOWL(Web Ontology Language) 은 가장 강력하다. 클래스 상속, 제약 조건, 자동 추론까지 지원한다. 법률 문서에 비유할 수 있다 — 모든 조항과 예외를 정밀하게 기술할 수 있는 대신 추론 엔진(Reasoner)을 별도로 띄워야 하고 학습 곡선이 가파르다. FIBO가 OWL인 이유도 금융 규제의 복잡성 때문이다.\nDataNexus가 하려는 건 추론이 아니다. \u0026ldquo;객단가가 뭔지, 어떤 테이블의 어떤 컬럼에 있는지\u0026quot;를 NL2SQL 엔진에 알려주는 맥락 제공이다. OWL은 과했다.\nRDFS(RDF Schema) 는 반대로 너무 가볍다. subClassOf 정도는 되는데 동의어나 용어 정의를 달 표준 속성이 없다.\nSKOS(Simple Knowledge Organization System) 가 딱 맞았다. 이름부터 \u0026ldquo;단순한 지식 조직 체계\u0026quot;다. 도서관 분류 체계나 시소러스(Thesaurus: 동의어·유의어·상하위어를 매핑해둔 용어 관계 사전)를 표현하려고 만든 W3C 표준인데 — DataNexus가 하는 일이 정확히 비즈니스 용어 사전 관리다. 핏이 맞을 수밖에 없다.\nSKOS 개념이 DataNexus 구조에 어떻게 대응되는지 정리하면:\nSKOS DataNexus (DataHub + DozerDB) 쉽게 말하면 skos:Concept Glossary Term / Entity 노드 용어 하나 skos:broader IsA 관계 (상위 개념) \u0026ldquo;객단가는 매출지표의 일종\u0026rdquo; skos:narrower IsA 역방향 (하위 개념) \u0026ldquo;매출지표의 하위에 객단가\u0026rdquo; skos:related RelatedTo 계열 \u0026ldquo;관련 있는 용어\u0026rdquo; * skos:prefLabel Term name (한국어 대표명) 공식 이름 skos:altLabel 동의어 (영문, 약어) \u0026ldquo;객단가\u0026rdquo; = \u0026ldquo;ATV\u0026rdquo;, \u0026ldquo;Average Transaction Value\u0026rdquo; skos:definition Term definition 용어 뜻풀이 skos:ConceptScheme 도메인별 용어 묶음 \u0026ldquo;유통 용어집\u0026rdquo;, \u0026ldquo;재무 용어집\u0026rdquo; * 주의할 점이 있다. skos:related는 양방향이다. \u0026ldquo;A related B\u0026quot;이면 자동으로 \u0026ldquo;B related A\u0026quot;도 성립한다. DozerDB의 SELLS나 SUPPLIED_BY 같은 관계는 방향이 있다. A매장이 B상품을 판매한다고 B상품이 A매장을 판매하진 않는다. 이 방향 정보는 SKOS로 내보낼 때 손실된다. 뒤에서 다시 다룬다.\nDozerDB 위에 SKOS를 얹다 원칙은 간단했다. 기존 그래프를 건드리지 않는다.\nDozerDB에 이미 들어간 MANUFACTURES, STOCKS, CALCULATED_FROM 같은 관계를 바꾸면 기존 쿼리가 전부 깨진다. 잘 돌아가는 구조를 표준 맞추겠다고 뒤집는 건 현장에서 가장 흔한 삽질이다.\n기존 노드 위에 SKOS 메타데이터를 오버레이 했다. 투명 필름 한 장 덮는 느낌이다.\n// 기존 Entity 노드에 SKOSConcept 라벨과 SKOS 속성을 추가 MATCH (net:Entity {name: \u0026#39;순매출\u0026#39;}) SET net:SKOSConcept SET net.skos_prefLabel = \u0026#39;순매출\u0026#39; SET net.skos_altLabel = [\u0026#39;Net Sales\u0026#39;, \u0026#39;순매출액\u0026#39;] SET net.skos_definition = \u0026#39;총매출에서 반품과 에누리를 차감한 금액\u0026#39; SET net.skos_inScheme = \u0026#39;finance-terms\u0026#39; 유통 도메인도 똑같다.\n// 유통 도메인 용어 예시 MATCH (atv:Entity {name: \u0026#39;객단가\u0026#39;}) SET atv:SKOSConcept SET atv.skos_prefLabel = \u0026#39;객단가\u0026#39; SET atv.skos_altLabel = [\u0026#39;ATV\u0026#39;, \u0026#39;Average Transaction Value\u0026#39;, \u0026#39;객단\u0026#39;] SET atv.skos_definition = \u0026#39;총매출액을 구매 고객수로 나눈 값\u0026#39; SET atv.skos_inScheme = \u0026#39;retail-terms\u0026#39; 기존 Entity 노드는 그대로다. SKOSConcept이라는 라벨과 skos_ 접두사 속성이 위에 붙을 뿐. 기존 Cypher 쿼리에는 영향이 없다.\nbroader/narrower 관계는 두 가지 방법이 있었다. BROADER, NARROWER 엣지를 IsA와 나란히 미리 만들어두거나, 기존 IsA 관계를 Export 시점에 skos:broader로 바꿔 출력하거나.\n후자를 택했다. 엣지를 이중으로 만들면 IsA가 바뀔 때마다 BROADER도 동기화해야 한다. 동기화가 어긋나면 데이터가 꼬인다. 원천(Source of Truth)은 하나여야 한다. Export 시점에 한 번 변환하는 게 단순하고 안전하다.\n가져오기와 내보내기 표준이 들어가면서 두 가지가 가능해졌다.\n가져오기 — FIBO에서 금융 용어를, GS1 GPC에서 상품 분류 체계를 DataNexus로 끌어오는 경우다. FIBO는 원래 OWL로 배포되지만 SKOS로 변환된 파생 버전도 있다. GPC도 마찬가지로 SKOS 매핑이 가능하다. \u0026ldquo;의류 → 여성복 → 원피스\u0026rdquo; 같은 상품 계층을 그대로 가져와서 유통 고객사 온톨로지의 뼈대로 쓸 수 있다. OWL의 복잡한 제약 조건은 빠지지만, DataNexus에 필요한 건 용어 이름·정의·상하위 관계뿐이다. SKOS 서브셋으로 충분하다.\n내보내기 — DataNexus 용어를 고객사 시스템으로 보내는 경우. DozerDB 그래프에서 특정 도메인(예: retail-terms)의 노드와 관계를 꺼내서 SKOS Turtle 포맷으로 변환한다.\n@prefix skos: \u0026lt;http://www.w3.org/2004/02/skos/core#\u0026gt; . @prefix dnx: \u0026lt;http://datanexus.ai/ontology/\u0026gt; . dnx:atv a skos:Concept ; skos:prefLabel \u0026#34;객단가\u0026#34;@ko ; skos:altLabel \u0026#34;ATV\u0026#34;@en, \u0026#34;Average Transaction Value\u0026#34;@en ; skos:definition \u0026#34;총매출액을 구매 고객수로 나눈 값\u0026#34;@ko ; skos:broader dnx:sales-metrics ; skos:inScheme dnx:retail-terms . dnx:sales-metrics a skos:Concept ; skos:prefLabel \u0026#34;매출지표\u0026#34;@ko ; skos:narrower dnx:atv, dnx:net-sales, dnx:upt ; skos:inScheme dnx:retail-terms . 유통 현장에서 \u0026ldquo;객단가\u0026quot;라고 부르는 걸 어떤 시스템에서는 \u0026ldquo;ATV\u0026quot;로, 어떤 곳에서는 \u0026ldquo;평균구매단가\u0026quot;로 부른다. altLabel에 이 별칭들을 다 넣어두면 NL2SQL 엔진이 어떤 이름으로 질문이 들어와도 같은 테이블을 찾을 수 있다. 이 파일을 Collibra든 TopBraid이든 SKOS를 지원하는 어떤 시스템에든 넣을 수 있다.\n가져오기/내보내기가 되면 앞서 얘기한 문제들이 풀린다. 유통 그룹에서 백화점은 \u0026ldquo;매출\u0026quot;을 점포별 POS 합산으로, 온라인몰은 결제 완료 기준으로, 마트는 반품 차감 후 기준으로 각각 정의하고 있다고 하자. 각 관계사가 DataNexus에 자기 용어를 SKOS로 내보내면, 그룹 본사에서 이걸 받아 매핑 테이블을 만들 수 있다. \u0026ldquo;백화점의 매출 = 온라인몰의 확정매출 = 마트의 순매출\u0026quot;이라는 관계가 표준 포맷으로 잡히는 거다. 금융 고객사라면 감독 기관에 용어 정의와 데이터 계보를 보고해야 할 때 SKOS Turtle 파일을 그대로 제출하거나, 기관이 요구하는 포맷으로 변환할 수 있다. 표준이 없으면 이런 건 전부 수작업이다.\nSchema.org 같은 RDFS/OWL 기반 표준은 이 SKOS 레이어 범위 밖이다. 필요해지면 별도 변환기를 만들면 되지만 당장 우선순위는 아니다.\n남은 한계 SKOS로 전부 해결되진 않는다.\nSKOS에는 레이블 자체에 메타데이터를 붙이는 확장(SKOS-XL)이 있다. \u0026ldquo;순매출\u0026quot;이라는 이름이 언제 등록됐는지, 누가 승인했는지를 기록할 수 있다. 다국어 레이블 관리가 복잡해지면 꺼내 써야 할 수도 있는데, 아직은 안 넣었다.\nOWL 수준의 추론도 SKOS 범위 밖이다. \u0026ldquo;A가 B의 하위이고, B가 C의 하위이면, A는 C의 하위다\u0026rdquo; 같은 자동 추론. 온톨로지 규모가 작을 땐 없어도 되는데, 수천 개 용어가 쌓이면 얘기가 달라질 수 있다.\n가장 아쉬운 건 커스텀 관계 Export다. DozerDB의 SELLS, STOCKS, SUPPLIED_BY 같은 유통 도메인 특화 관계는 SKOS 표준에 대응하는 게 없다. \u0026ldquo;A매장이 B상품을 판매한다\u0026quot;는 방향이 있는 관계인데, skos:related로 뭉뚱그리면 방향과 의미가 사라진다. dnx:sells 같은 커스텀 네임스페이스로 확장하면 정보는 보존되는데, 받는 쪽이 이 커스텀 관계를 이해할 수 있어야 한다. 정보 손실 vs 호환성 — 트레이드오프다.\n커스텀 관계를 내보내는 구체적인 방법 skos:related로 뭉뚱그리면 의미가 사라진다고 했다. 그래서 어떻게 하느냐.\nDataNexus 전용 네임스페이스를 정의한다.\n@prefix dnx: \u0026lt;http://datanexus.ai/ontology/relation/\u0026gt; . dnx:atv-store a skos:Concept ; skos:prefLabel \u0026#34;객단가-매장 관계\u0026#34;@ko ; dnx:relationshipType \u0026#34;SoldBy\u0026#34; ; dnx:direction \u0026#34;outgoing\u0026#34; ; dnx:confidence 0.95 ; dnx:validFrom \u0026#34;2024-01-01\u0026#34; . dnx:relationshipType, dnx:direction, dnx:confidence 같은 커스텀 속성으로 DozerDB의 SELLS 관계가 가진 방향성과 메타데이터를 보존한다. 받는 쪽 시스템이 dnx: 네임스페이스를 이해하면 정보 손실 없이 복원할 수 있고, 이해 못 하면 skos:related로 폴백한다. 정보가 사라지는 게 아니라 읽을 수 있는 시스템에서만 보이는 거다.\n현실적으로는 이렇게 운영한다.\nExport 대상 방식 정보 보존율 SKOS 네이티브 시스템 (Collibra, TopBraid) skos: 표준 속성만 포함 ~80% (방향, 속성 손실) DataNexus 간 교환 (그룹사 ↔ 그룹사) dnx: 커스텀 네임스페이스 포함 ~95% (거의 완전 보존) 규제 보고용 skos: + skos:note에 커스텀 관계 텍스트 기록 ~85% (사람이 읽을 수 있는 수준) DataHub 쪽에서는 Export 시점에 미매핑 속성을 처리하는 규칙도 정해 뒀다.\nDozerDB 속성 SKOS Export 시 처리 confidence dnx:confidence (커스텀) 또는 skos:note에 텍스트로 기록 since / valid_until dnx:validFrom / dnx:validUntil 또는 skos:historyNote cardinality dnx:cardinality (커스텀 전용, SKOS에 대응 없음) operator (CALCULATED_FROM) dnx:calculationOperator 완벽하진 않다. dnx: 네임스페이스는 DataNexus 생태계 안에서만 의미가 있고, 외부 시스템이 이걸 해석하리라는 보장은 없다. 표준의 한계를 커스텀 확장으로 메꾸면 결국 새로운 비표준을 만드는 셈이다. 이 지점에서 더 나아가려면 SKOS-XL이나 별도의 Application Profile을 정의해야 하는데, 지금은 과하다. 필요해지면 그때 넣는다.\n80%는 SKOS 표준으로 커버하고, 20%는 DozerDB 커스텀 관계로 보완한다. 표준이 못 담는 부분은 내부 확장으로 채우되, 무리해서 표준 안에 구겨넣지 않는다.\n다음 글 온톨로지를 만들었는데, 이게 제대로 된 건지 어떻게 알까. 다음 글에서는 CQ(Competency Questions)로 온톨로지를 사전 검증하는 방법을 다룬다.\nDataNexus를 설계하고 구축하는 과정을 기록합니다. GitHub | LinkedIn\n","permalink":"https://biz-agentic-ai.github.io/posts/datanexus/004-skos-compatibility-layer/","summary":"DataNexus 온톨로지를 외부와 연결하기 위해 SKOS를 선택한 이유. LPG와 RDF, 두 그래프 모델을 잇는 호환 레이어 설계.","title":"4. SKOS 호환 레이어를 왜 넣었는가"},{"content":"\u0026ldquo;스타스키마 안 해도 되나요?\u0026rdquo; 클라우드 DW 전환 프로젝트를 하면 꼭 나오는 질문이다.\n온프레미스에서 수년간 운영하던 DW를 BigQuery나 Azure Synapse로 옮기는 자리. 누군가가 묻는다. \u0026ldquo;거기는 Columnar Storage(컬럼기반 저장소)라 조인 비용이 다르다는데, 그러면 스타스키마 안 해도 되는 거 아닌가요?\u0026rdquo;\n답을 먼저 말하면 - 상황에 따라 다르다. 그런데 이 \u0026ldquo;상황에 따라 다르다\u0026quot;가 구체적으로 뭘 따져야 하는 건지를 아는 사람은 많지 않다.\n온프레미스 시절의 공식 Kimball의 Dimensional(Star-Schema) 모델링이 사실상 표준이던 시절이 있었다.\n이유는 단순했다. 디스크 I/O가 비쌌고, 조인은 더 비쌌다. Row 기반 스토리지에서 테이블 10개를 조인하면 쿼리 응답이 분 단위로 넘어갔다. 그래서 미리 조인해 놓는 게 합리적이었다.\n모델링 의사결정이 곧 성능 의사결정이었다. 어디까지 비정규화할 것인가, 집계 테이블을 몇 단계로 쌓을 것인가, 파티션 키를 뭘로 잡을 것인가. 이 결정들이 쿼리 응답 시간을 초 단위에서 분 단위로 갈랐다.\nKimball의 방법론은 이 제약 안에서 비즈니스 가독성과 쿼리 성능을 동시에 잡으려는 시도였다. 부서마다 제각각이던 차원 테이블을 전사 공통 기준으로 통일(Conformed Dimension)해서 일관성을 보장하고, Bus Matrix로 전사 통합을 설계하고. 방법론 자체의 완성도는 지금 봐도 높다.\n문제는 이 방법론이 만들어진 전제가 바뀌었다는 거다.\n클라우드가 바꾼 전제들 클라우드 DW로 오면서 물리적 제약이 근본적으로 달라졌다.\nColumnar Storage. BigQuery, Redshift, Synapse 모두 컬럼 기반이다. SELECT에서 필요한 컬럼만 읽는다. 컬럼이 수백 개인 테이블에서 서너 개만 조회하면 그것만 스캔한다. Row 기반에서는 전부 읽어야 했다.\nCompute/Storage 분리. 스토리지가 싸졌다. 중복 저장해도 비용 부담이 작다. 온프레미스에서 디스크 용량 아끼려고 정규화를 고민하던 것과는 상황이 다르다.\nMPP 아키텍처. 대규모 병렬 처리가 기본이다. 조인 비용이 온프레미스 RDBMS 대비 상대적으로 낮아졌다. 물론 공짜는 아니다 - 셔플이 발생하면 여전히 느리다. 하지만 \u0026ldquo;조인은 무조건 피하라\u0026quot;는 원칙이 더 이상 절대적이지 않다.\nELT 패러다임. 원본을 먼저 적재하고, DW 안에서 변환한다. 변환 로직을 DW 엔진의 컴퓨팅 파워로 처리한다. ETL 시절처럼 변환 서버를 별도로 두고 \u0026ldquo;다 만들어서 넣는\u0026rdquo; 구조가 아니다.\n반정형 데이터 지원. JSON, ARRAY, STRUCT를 네이티브로 다룬다. 전통적인 관계형 모델로 풀기 어려웠던 유연한 스키마를 DW 안에서 직접 처리할 수 있다.\n이 변화들이 기존 모델링 원칙의 근거를 흔든다. 하지만 근거가 약해진 것과 원칙이 틀린 것은 다른 이야기다.\n세 가지 선택지 클라우드 DW에서 자주 비교되는 세 가지 접근법이 있다.\n1. Kimball Dimensional 모델링 스타스키마, 팩트와 디멘션, Conformed Dimension. 여전히 가장 널리 쓰인다.\n클라우드에서도 유효한 이유가 있다. 비즈니스 사용자가 이해하기 쉽다. \u0026ldquo;매출 팩트를 고객 디멘션으로 잘라본다\u0026quot;는 구조는 BI 도구와도 궁합이 좋다. Power BI, Tableau, Looker 전부 이 구조를 전제로 최적화되어 있다.\n달라진 것도 있다. 집계 테이블을 미리 쌓아둘 필요가 줄었다. 컬럼나 스토리지에서 원본 팩트를 바로 집계해도 충분히 빠르다. SCD Type 2 같은 이력 관리도 스토리지 부담이 작아져서 더 적극적으로 쓸 수 있게 됐다.\n약점은 유연성이다. 스키마가 바뀌면 팩트/디멘션 구조를 재설계해야 한다. 애자일하게 빠르게 바뀌는 요구사항에 대응하기가 구조적으로 어렵다.\n2. Data Vault 2.0 Hub(비즈니스 키), Satellite(속성), Link(관계). 원본 데이터를 있는 그대로 이력과 함께 저장하는 데 초점을 맞춘 방법론이다.\n강점은 명확하다. 감사 추적성(auditability). 원본이 언제 어떤 값이었는지를 완전하게 보존한다. 소스 시스템이 추가되거나 스키마가 변경되어도 기존 구조에 영향을 주지 않는다. 병렬 적재가 가능해서 ELT 패러다임과도 잘 맞는다.\n현실적인 걸림돌이 있다. 직접 쿼리하기가 까다롭다. Hub-Satellite 조인을 여러 번 거쳐야 하나의 비즈니스 엔터티가 나온다. 그래서 결국 프레젠테이션 레이어(보통 스타스키마)를 따로 만들어야 한다. 모델링 레이어가 하나 더 생기는 셈이다. 팀이 Data Vault 경험이 없으면 학습 곡선도 가파르다.\n금융, 의료 같은 규제 산업에서 감사 추적이 필수일 때, 또는 소스 시스템이 수시로 추가되는 환경에서 강하다.\n3. One Big Table (OBT) 팩트와 디멘션을 전부 하나의 와이드 테이블에 합친다. 극단적인 비정규화다.\n클라우드에서 이게 통하는 이유가 있다. 컬럼나 스토리지에서는 200개 컬럼이 있어도 쿼리에 쓰는 5개만 읽으니까 성능 저하가 크지 않다. 조인이 없으니 쿼리가 단순하다. 개발 속도도 빠르다. dbt 같은 도구에서 한 번의 SELECT로 만들면 끝이다.\n대가가 있다. 데이터 정합성을 보장할 구조적 장치가 없다. 고객 주소가 바뀌면 모든 OBT를 다시 빌드해야 한다. 같은 디멘션 속성이 여러 OBT에 중복되면 어디가 맞는 건지 알 수 없다. 데이터가 적고 도메인이 단순할 때는 빠르지만, 규모가 커지면 관리가 급격히 어려워진다.\n프로토타이핑이나 단일 도메인의 분석용 마트로는 좋다. 전사 DW의 기반 구조로 쓰기에는 위험하다.\n실무에서의 판단 기준 \u0026ldquo;뭐가 최고냐\u0026quot;는 의미 없는 질문이다. 아래 기준으로 따져야 한다.\n팀 역량. Data Vault를 제대로 하려면 방법론을 아는 사람이 팀에 있어야 한다. 없으면 Kimball이 현실적이다. OBT는 진입 장벽이 낮지만 규모가 커지면 경험 있는 모델러가 더 절실해진다.\n데이터 복잡도. 소스 시스템이 3개인가 30개인가. 도메인이 하나인가 여러 개인가. 복잡도가 높을수록 Kimball의 Conformed Dimension이나 Data Vault의 Hub 구조가 필요하다.\n변경 빈도. 요구사항이 자주 바뀌는 환경이면 Data Vault가 유리하다. 안정적인 환경이면 Kimball로 충분하다.\n규제 요건. 감사 추적이 법적으로 필요하면 Data Vault를 고려해야 한다. 아니라면 오버엔지니어링이 될 수 있다.\n쿼리 패턴. BI 대시보드 중심이면 Kimball 구조가 BI 도구와 궁합이 좋다. Ad-hoc 분석이 많으면 OBT의 단순함이 장점이 된다.\n실무에서 자주 보는 패턴은 레이어드 접근이다.\nRaw (원본 적재) → Staging (정제) → Integration (통합 모델) → Mart (분석용) Integration 레이어를 Data Vault로 설계하고, Mart를 스타스키마로 제공하는 조합이 대표적이다. Data Vault는 허브-링크-위성 구조로 이력 추적과 유연한 확장에 특화된 모델이고, 스타스키마는 중심 팩트 테이블 주위에 차원 테이블을 별 모양으로 배치한 분석 최적화 모델이다. 또는 Integration을 중복 없이 정규화한 관계형 모델(3NF)로 잡고 Mart를 Kimball 방식으로 가는 전통적인 구조도 있다. 어느 쪽이든 핵심은 레이어를 나누는 것이다.\n한 레이어에서 원본 보존과 분석 최적화를 동시에 해결하려는 순간 복잡도가 폭발한다.\n모델링은 여전히 중요하다 클라우드로 오면서 바뀐 건 \u0026ldquo;왜 이렇게 모델링하는가\u0026quot;의 판단 기준이지, \u0026ldquo;모델링이 필요한가\u0026quot;의 여부는 아니다.\n스토리지가 싸졌으니 비정규화를 더 적극적으로 해도 된다. 조인 비용이 줄었으니 집계 테이블을 덜 만들어도 된다. 하지만 비즈니스 용어의 일관성, 데이터 계보(lineage), 도메인 간 통합 - 이런 문제는 인프라가 바뀐다고 사라지지 않는다.\n오히려 클라우드에서 모델링 없이 시작한 팀이 나중에 더 고생한다. OBT로 빠르게 만들어서 처음엔 잘 돌아간다. 1년 지나면 같은 지표의 정의가 테이블마다 다르고, 어디가 원본인지 아무도 모른다. 기술 부채는 조용히 쌓인다.\n도구와 인프라가 좋아진 만큼 모델링의 무게중심이 \u0026ldquo;성능 최적화\u0026quot;에서 \u0026ldquo;의미 체계의 일관성\u0026quot;으로 옮겨가고 있다. 결국 DataNexus에서 온톨로지로 풀려는 문제와 같은 맥락이다 - 기계가 읽을 수 있는 비즈니스 맥락이 없으면, 아무리 좋은 인프라 위에서도 데이터는 의미를 잃는다.\n","permalink":"https://biz-agentic-ai.github.io/guides/dw-modeling/001-cloud-era-dw-modeling/","summary":"Synapse, BigQuery, Redshift로 오면서 DW 모델링 관점이 어떻게 달라졌는지. Kimball, Data Vault, One Big Table - 실무에서의 판단 기준.","title":"1. 클라우드 DW에서 Kimball은 여전히 유효한가"},{"content":"\rGoogle Colab에서 실습하기\r왜 Glossary를 온톨로지로 쓰려 했나 DataNexus의 핵심 아이디어는 단순하다. 비즈니스 용어 사이의 관계를 그래프로 정의해두면, NL2SQL 엔진이 그 그래프를 참조해서 자연어를 SQL로 바꿀 수 있다. 이 그래프가 온톨로지다.\n온톨로지라고 하면 학술 논문에나 나올 것 같은데, 실체는 별거 없다. \u0026ldquo;순매출은 매출의 한 종류다(IsA)\u0026rdquo;, \u0026ldquo;매출은 총매출, 반품, 에누리를 포함한다(HasA)\u0026rdquo;. 사람 머릿속에 있는 업무 지식을 기계가 읽을 수 있게 옮긴 것뿐이다.\n이걸 어디에 저장할지가 고민이었다. 온톨로지 전용 시스템을 하나 더 띄우면 관리 포인트가 늘어난다. 이미 DataHub를 메타데이터 플랫폼으로 쓰고 있었고, 거기 Business Glossary 가 딸려 있었다. 용어 등록, 관계 설정 다 된다. 이전 글에서 Glossary의 관계 4종(IsA, HasA, RelatedTo, Values)이면 비즈니스 용어 계층구조를 충분히 표현할 수 있다고 봤었다.\n시스템 하나를 줄일 수 있다는 게 매력적이었다. GraphQL API로 프로그래밍 접근도 되고, 용어가 변경되면 Kafka MCL(Metadata Change Log) 이벤트가 자동으로 나간다. 나쁘지 않은 출발점이었다.\nGlossary에 용어를 넣기 시작했다 DataHub 세팅하고 제일 먼저 한 게 Glossary Term 등록이었다. \u0026ldquo;순매출 IsA 매출\u0026rdquo;, \u0026ldquo;매출 HasA 총매출, 반품, 에누리\u0026rdquo;. 이런 식으로 용어를 넣고 관계를 걸었다.\n기본 계층구조는 깔끔하게 들어갔다. 매출 → 총매출, 순매출 → 실매출.\n문제는 그 다음이었다.\n관계 4종의 한계: \u0026ldquo;공장과 제품\u0026rdquo; 실제 업무 데이터를 모델링하면서 벽에 부딪혔다.\n\u0026ldquo;A 공장에서 생산된 B 제품\u0026quot;과 \u0026ldquo;A 공장에 재고가 있는 B 제품\u0026rdquo;. 둘 다 공장과 제품 사이의 관계인데, 하나는 생산(Manufactures), 하나는 재고(Stocks)다. 의미가 완전히 다르다.\nDataHub Glossary에서 이걸 표현하면? 둘 다 RelatedTo가 된다. \u0026ldquo;공장 RelatedTo 제품\u0026quot;이 두 개 생기는데, 어느 게 생산이고 어느 게 재고인지 구분할 방법이 없다.\n이게 왜 치명적이냐면—DataNexus의 NL2SQL 엔진이 온톨로지를 보고 SQL을 만들기 때문이다. \u0026ldquo;A 공장에서 생산된 제품 목록 보여줘\u0026quot;라는 질문이 들어오면, 엔진은 공장-제품 관계를 찾아서 해당 테이블과 JOIN 경로를 결정한다.\n사용자 질문: \u0026ldquo;A 공장에서 생산된 제품은?\u0026quot;\n온톨로지 조회: 공장 → RelatedTo → 제품 (생산? 재고? 알 수 없음)\n→ LLM이 production 테이블 대신 inventory 테이블을 JOIN할 수 있음 → 잘못된 결과 반환\n관계 유형이 RelatedTo 하나뿐이니 엔진한테는 판단 근거가 없다. 잘못된 JOIN을 타면 사용자에게 엉뚱한 데이터가 나간다.\n확장하려면 재배포가 필요하다 DataHub에서 관계를 세분화하면 되지 않느냐. 안 된다.\nPDL(Persona Data Language)로 새 Aspect를 정의하고, @Relationship 어노테이션으로 관계 유형을 선언하고, DataHub를 빌드해서 재배포해야 한다. 관계 유형 하나 추가할 때마다 이 사이클을 돌려야 한다.\n비즈니스 모델링을 하다 보면 관계는 계속 늘어난다. \u0026ldquo;공급(Supplies)\u0026rdquo;, \u0026ldquo;검수(Inspects)\u0026rdquo;, \u0026ldquo;반품(Returns)\u0026rdquo;\u0026hellip; 업무 맥락에 따라 수십 가지가 필요해지는데, 하나마다 코드 고치고 재배포하는 건 현실적이지 않다.\n파고 들어가니 더 나왔다 관계 유형만 문제가 아니었다.\n동의어 충돌 \u0026ldquo;순매출\u0026quot;과 \u0026ldquo;실매출\u0026quot;을 동의어로 등록했다. 같은 개념인데 이름만 다른 경우다. 그런데 두 용어 모두 \u0026ldquo;Net Sales\u0026quot;라는 영문 동의어를 갖고 있었다. 하나의 영문명에 한글 용어 두 개가 매핑된 상황—DataHub는 이걸 그냥 넘긴다. 경고도 없다.\nNL2SQL에서 동의어 매핑이 꼬이면 엔진이 엉뚱한 용어를 참조한다. 용어가 수백 개를 넘어가면 이런 충돌을 사람 눈으로 잡을 수 없다. 커스텀 검증 로직을 따로 짜야 한다는 뜻이다.\n시각화 DataHub UI는 데이터 계보(Lineage) 탐색에 맞춰져 있다. 테이블 A → 테이블 B로 데이터가 흐르는 방향성 있는 트리.\n온톨로지는 구조가 다르다. 노드 수십~수백 개가 다대다로 엮인 그물망이다. \u0026ldquo;제품\u0026quot;이 \u0026ldquo;공장\u0026rdquo;, \u0026ldquo;창고\u0026rdquo;, \u0026ldquo;거래처\u0026rdquo;, \u0026ldquo;카테고리\u0026quot;와 전부 다른 관계로 연결되어 있고, 그 노드들끼리 또 서로 물려 있다. DataHub에는 이런 그래프를 탐색할 화면 자체가 없다.\n만들어 놓고 전체 그림을 못 보면 관리가 안 된다.\n관계에 속성을 붙일 수 없다 이게 제일 문제였다.\nDataHub Glossary에서 \u0026ldquo;A RelatedTo B\u0026quot;를 설정하면 그 관계에 아무것도 더 달 수 없다. 실무에서는 관계 자체에 정보가 필요한 경우가 많다.\n신뢰도(confidence) 가 대표적이다. 자동 추출된 관계는 0.7, 전문가가 직접 정의한 관계는 0.95—이 차이를 NL2SQL 엔진이 알아야 한다. 유효 기간(temporal) 도 마찬가지다. 조직 개편으로 부서-제품 매핑이 바뀌면, \u0026ldquo;이 관계가 언제부터 언제까지 유효한지\u0026quot;를 추적해야 한다. 카디널리티(cardinality) 는 JOIN 전략에 직접 영향을 준다.\n조직 개편으로 부서-제품 매핑이 바뀌면, 과거 시점 조직 구조로 현재 데이터를 조회하게 된다. 리포트 수치가 안 맞는 전형적인 원인이다. 관계에 시간축이 없으면 이걸 막을 방법이 없다.\n정리: 되는 것과 안 되는 것 되는 것 안 되는 것 용어 정의 (name, definition) 세분화된 관계 유형 (MANUFACTURES, STOCKS 등) 동의어 등록 (커스텀 필드) 동의어 중복/충돌 자동 감지 4종 관계 (IsA, HasA, RelatedTo, Values) 관계에 속성 부여 (신뢰도, 유효 기간) GraphQL API로 프로그래밍 접근 복잡한 그래프 탐색 UI Kafka MCL 이벤트 스트림 재배포 없는 실시간 관계 유형 확장 DataHub Glossary는 용어 사전으로서는 쓸 만하다. 온톨로지 저장소로는 표현력이 모자랐다.\n역할을 나눴다: DataHub + DozerDB Glossary를 통째로 버리는 건 답이 아니었다. 용어 정의의 원천(Source of Truth)으로 DataHub를 대체할 게 없다. GraphQL API, Kafka MCL 이벤트—이 인프라를 다른 도구에서 바닥부터 만드는 건 시간 낭비다.\n각자 잘하는 걸 맡겼다.\nDataHub Glossary → 용어 정의와 기본 관계의 원천 (Source of Truth) DozerDB → 세분화된 관계, 속성 달린 엣지, 그래프 추론 담당 DozerDB를 고른 건 Cypher 쿼리를 쓸 수 있어서다. 관계(엣지)에 속성을 자유롭게 붙일 수 있고, 관계 유형을 추가할 때 스키마 변경이나 재배포가 필요 없다.\n동기화 흐름은 단순하다. DataHub에서 Glossary Term이 바뀌면 Kafka MCL 이벤트가 나간다. 이벤트를 구독하는 Consumer가 DozerDB 온톨로지 그래프에 반영한다. 이름이나 정의 같은 기본 정보는 DataHub가 쥐고 있고, DozerDB는 그 위에 세분화된 관계와 속성을 얹는다.\nDozerDB에서의 관계 정의 아까 문제됐던 \u0026ldquo;공장-제품\u0026rdquo; 관계가 DozerDB에서는 이렇게 풀린다.\n// 엔티티 생성 (DataHub에서 동기화된 용어) CREATE (factory:Entity {name: \u0026#39;A공장\u0026#39;, type: \u0026#39;Factory\u0026#39;}) CREATE (product:Entity {name: \u0026#39;B제품\u0026#39;, type: \u0026#39;Product\u0026#39;}) // 생산 관계 — 시작 시점과 신뢰도를 속성으로 기록 CREATE (factory)-[:MANUFACTURES { since: \u0026#39;2024-01-01\u0026#39;, confidence: 0.95 }]-\u0026gt;(product) // 재고 관계 — 별도 엣지, 수량과 갱신 시점 CREATE (factory)-[:STOCKS { quantity: 500, last_updated: \u0026#39;2026-02-01\u0026#39; }]-\u0026gt;(product) MANUFACTURES와 STOCKS가 별개의 관계 유형이다. \u0026ldquo;A 공장에서 생산된 제품\u0026quot;이라는 질문이 오면 엔진이 MANUFACTURES를 찾아서 production 테이블로 정확히 JOIN한다. RelatedTo 하나로 퉁치던 것과는 근본적으로 다르다.\n파생 지표도 그래프에 넣었다 파생 지표 정의를 Excel로 관리하면 원본 용어가 바뀔 때 파생 시트가 안 따라간다. \u0026ldquo;순매출 = 총매출 - 반품 - 에누리\u0026quot;에서 총매출 정의가 바뀌었는데 순매출 쪽은 그대로—이런 불일치가 리포트까지 올라간다.\n이번에는 CALCULATED_FROM 관계로 계산식 자체를 그래프에 넣었다.\n// 순매출의 계산 구조를 관계로 표현 MATCH (net:Entity {name: \u0026#39;순매출\u0026#39;}) MATCH (gross:Entity {name: \u0026#39;총매출\u0026#39;}) MATCH (returns:Entity {name: \u0026#39;반품\u0026#39;}) MATCH (discounts:Entity {name: \u0026#39;에누리\u0026#39;}) CREATE (net)-[:CALCULATED_FROM {operator: \u0026#39;subtract\u0026#39;}]-\u0026gt;(gross) CREATE (net)-[:CALCULATED_FROM {operator: \u0026#39;subtract\u0026#39;}]-\u0026gt;(returns) CREATE (net)-[:CALCULATED_FROM {operator: \u0026#39;subtract\u0026#39;}]-\u0026gt;(discounts) 계산식이 바뀌면 관계를 수정한다. 변경 이력은 그래프 DB가 추적한다. Excel 시트 어딘가에 묻혀서 누가 언제 고쳤는지 모르는 것보다 낫다.\nSource of Truth가 깨진 건 아닌가 여기서 한 가지 짚고 넘어갈 게 있다.\n1편에서 메타데이터 카탈로그를 \u0026ldquo;온톨로지의 원천(Source of Truth)\u0026ldquo;이라고 썼다. 2편에서도 DataHub Glossary의 관계 4종이면 충분하다고 봤다. 그런데 3편에 와서 DozerDB를 추가했다. \u0026ldquo;그러면 Source of Truth가 두 개가 된 거 아닌가?\u0026rdquo; 당연한 질문이다.\n답부터 말하면, SoT의 대상이 달라진 거다.\nSoT라는 개념은 \u0026ldquo;모든 것을 하나의 시스템에 넣는다\u0026quot;가 아니다. \u0026ldquo;특정 데이터 카테고리에 대해 어디가 최종 권위를 가지느냐\u0026quot;를 정하는 거다. DataHub와 DozerDB는 서로 다른 질문에 답한다.\n\u0026ldquo;순매출이 뭐야?\u0026rdquo; → DataHub가 답한다. 이름, 정의, 동의어, 소유 부서. 용어의 정체성에 관한 건 DataHub가 최종 권위다. \u0026ldquo;순매출이 어떤 테이블과 어떤 경로로 연결되어 있어?\u0026rdquo; → DozerDB가 답한다. CALCULATED_FROM, MANUFACTURES 같은 세분화된 관계, 신뢰도, 유효 기간. 용어 간 연결의 의미론은 DozerDB가 쥐고 있다. 둘 사이에 충돌이 생기면? DataHub가 이긴다. DozerDB의 노드 이름이나 정의가 DataHub Glossary와 다르면 DataHub 쪽이 정답이다. Kafka MCL 이벤트가 이 방향으로만 흐른다 — DataHub → DozerDB. 역방향 동기화는 없다.\n처음부터 이 구분을 명확히 했어야 한다. 1편에서 \u0026ldquo;온톨로지의 원천\u0026quot;이라고 쓸 때, 실제로는 \u0026ldquo;용어 정의의 원천\u0026quot;이라고 써야 했다. 용어 정의와 관계 의미론을 하나의 시스템이 전부 감당할 수 있을 거라는 초기 가정이 틀렸다. 3편에서 그게 드러난 거고, DataHub + DozerDB 이중 구조는 그 결과다.\nSoT가 깨진 게 아니라 SoT의 범위가 좁아진 것이다.\n남은 문제: 표준 호환 DataHub Glossary 모델은 DataHub만의 구조다. 업계에는 FIBO(금융), Schema.org(범용) 같은 표준 온톨로지가 있다. 산업 표준을 가져오거나 DataNexus 온톨로지를 밖으로 내보내려면 표준 포맷을 지원해야 하는데, 지금 구조로는 DataNexus 안에서만 통하는 독자 체계다.\n외부 상호운용성 없는 온톨로지는 쓰임이 제한된다.\n다음 글에서 SKOS 호환 레이어를 왜 넣었는지 다룬다.\nGoogle Colab에서 실습하기\rDataNexus를 설계하고 구축하는 과정을 기록합니다. GitHub | LinkedIn\n","permalink":"https://biz-agentic-ai.github.io/posts/datanexus/003-datahub-glossary-as-ontology/","summary":"DataHub의 Business Glossary를 온톨로지 저장소로 쓰려고 했다. 되는 것과 안 되는 것, 그리고 우회한 방법.","title":"3. DataHub Glossary를 온톨로지로 쓸 수 있을까"},{"content":"비교 후보군 후보가 너무 많았다.\n메타데이터 카탈로그만 해도 DataHub, Amundsen, Apache Atlas, OpenMetadata. 상용까지 포함하면 Collibra, Alation도 있다. NL2SQL 엔진, 문서 지식엔진, 그래프 DB까지 네 축을 채워야 하는데 조합이 기하급수적으로 불어났다.\n엑셀 시트에 비교표를 만들었다. 행이 후보 도구, 열이 평가 기준. 3주쯤 지나니까 탭이 7개로 늘어나 있었다. 선택지가 많으면 안 고르는 게 문제다. 하나를 고르면 나머지와의 조합이 바뀌고, 다시 처음부터 비교해야 한다.\n네 가지 컴포넌트, 각각의 요건 이전 글에서 DataNexus의 네 가지 컴포넌트를 정의했다. 메타데이터 카탈로그, NL2SQL 엔진, 문서 지식엔진, 그래프 DB.\n양보할 수 없는 공통 기준이 세 가지 있었다. 오픈소스일 것. 멀티테넌시를 지원하거나 구현 가능할 것 — 그룹사별 데이터 격리는 필수다. 프로덕션 레디일 것 — 커뮤니티 활성도, 릴리즈 주기, 문서화 수준까지 봤다.\n컴포넌트마다 추가 요건도 달랐다. 메타데이터 카탈로그는 Business Glossary에서 용어 간 관계를 정의할 수 있어야 하고, 변경 이벤트를 실시간으로 내보낼 수 있어야 했다. NL2SQL 엔진 쪽은 사용자별 컨텍스트 분리와 Row-level Security. 문서 지식엔진은 벡터 검색만으로 안 되고 그래프 검색까지 하이브리드로 돌려야 했다. 그래프 DB는 Multi-DB와 Cypher 쿼리 지원이 전제였다.\n이 기준을 들고 후보를 걸렀다.\n메타데이터 카탈로그 DataHub, OpenMetadata, Amundsen, Apache Atlas, 상용(Collibra/Alation). 다섯을 놓고 봤다.\n상용은 먼저 빠졌다. 라이선스 비용도 문제지만 이 프로젝트에서 필요한 건 카탈로그의 Glossary를 온톨로지 저장소처럼 쓰는 거다. 상용 Glossary가 강력하긴 한데 내부 데이터 모델에 접근해서 커스터마이징하는 데 한계가 있다.\nApache Atlas는 Hadoop 생태계에 묶여 있다. HBase, Solr, Kafka를 전부 띄워야 한다. 2016년 설계 그대로인데 클라우드 네이티브 환경에서 돌리기엔 무겁다. Amundsen은 검색 중심 카탈로그로는 괜찮은데 Glossary에서 용어 간 관계를 정의하는 기능이 빈약하다. 온톨로지 저장소로 쓸 수 없었다.\n끝까지 고민한 건 OpenMetadata다. 아키텍처가 깔끔하고 데이터 품질 측정이 내장돼 있어서 단독 카탈로그로는 훌륭했다. 문제는 Glossary 관계가 Parent-Child와 RelatedTerms 위주라는 점. 상속(IsA)과 포함(HasA)을 명확히 구분해야 하는 온톨로지 표현에는 모자랐다. 실시간 이벤트 동기화도 웹훅 방식이라 대규모 스트리밍에서 Kafka 네이티브 대비 신뢰성이 떨어졌다.\nDataHub를 고른 이유는 명확하다.\nGlossary 관계가 4종이다. IsA(상속), HasA(포함), Values(값 목록), RelatedTo(일반 연관). 이 네 가지면 비즈니스 용어 간 계층을 표현할 수 있다. \u0026ldquo;순매출 IsA 매출\u0026rdquo;, \u0026ldquo;매출 HasA 총매출, 반품, 에누리\u0026rdquo; 같은 식이다.\nGraphQL API도 한몫했다. 메타데이터를 프로그래밍 방식으로 읽고 쓸 수 있어야 NL2SQL 엔진의 RAG Store에 온톨로지를 자동 동기화하는데, GraphQL이면 필요한 필드만 골라서 가져온다.\n결정적이었던 건 Kafka MCL 이벤트다. Metadata Change Log를 Kafka로 내보내는 구조인데, Glossary Term이 바뀌면 이벤트가 발행된다. 이걸 구독해서 그래프 DB 온톨로지를 실시간 동기화할 수 있다. 메타데이터 변경을 수동으로 반영하는 건 규모가 커질수록 반드시 누락이 생긴다. 양보할 수 없는 요건이었다.\nNL2SQL 엔진 처음에는 직접 만들까 생각했다. 대화형 BI 솔루션을 구축하면서 NL2SQL에 GPT와 Gemini를 붙이고, 프롬프트 엔지니어링을 최적화하고, 멀티에이전트 아키텍처까지 설계하는 데까지 갔었다.\n거기서 배운 게 두 가지다. DDL만으로는 LLM이 비즈니스 맥락을 이해할 수 없다는 것. 그리고 처음부터 만들면 사용자 인증, 쿼리 로깅, 데이터 필터링, 응답 스트리밍, 쿼리 학습까지 부수 기능이 한없이 불어난다는 것. 견적을 내보니 1개월 넘게 잡아먹힐 판이었다.\n그때 Vanna가 2.0으로 올라왔다.\n1.x는 단순했다. Python 클래스 하나 상속받아서 train(), ask() 호출하는 방식. 프로토타이핑엔 괜찮은데 프로덕션에 넣기엔 부족했다. 사용자별 컨텍스트 분리도 안 되고 보안 기능도 없었다.\n2.0은 다른 물건이다. Agent 기반 아키텍처로 바뀌면서 독립적인 구성 요소를 조합하는 방식이 됐고, 모든 컴포넌트에 사용자 ID가 자동 전파되는 User-Aware 구조가 들어갔다. Row-level Security가 프레임워크 수준에서 지원된다. 성공한 쿼리를 자동 학습하는 Tool Memory도 내장. 테이블이나 차트 같은 Rich UI Component를 실시간 전송하는 Streaming까지 갖췄다.\nUser-Aware와 Row-level Security가 가장 중요했다. DataNexus는 그룹사별로 데이터를 격리해야 하는데 NL2SQL 엔진 레벨에서 이걸 지원한다는 건 직접 구현할 코드가 대폭 줄어든다는 뜻이다.\nTool Memory도 컸다. NL2SQL 정확도를 올리는 가장 확실한 방법 중 하나가 성공 쿼리를 축적해서 유사 질문에 재활용하는 건데 이게 프레임워크에 내장돼 있다. 별도로 만들면 쿼리 저장, 유사도 매칭, 버전 관리까지 만져야 하는데 그 공수가 통째로 빠진다.\n문서 지식엔진 벡터 검색만으로는 부족하다.\n사업보고서나 내부 정책문서를 검색할 때 벡터 유사도만으로 청크를 가져오면 맥락이 끊긴다. \u0026ldquo;A사업부의 매출 인식 기준\u0026quot;을 찾고 싶은데 벡터 검색은 \u0026ldquo;매출\u0026quot;이 포함된 청크를 유사도 순으로 나열할 뿐이다. A사업부와 매출 인식 기준의 관계라든가, 기준이 언제 바뀌었는지 같은 그래프 구조 정보는 벡터에 안 담긴다.\nApeRAG는 세 가지 검색을 조합해서 이 문제를 푼다. 임베딩 기반 의미 검색인 Vector Search, 고유명사나 코드명처럼 문자열 자체가 중요한 Full-text Search, 문서에서 추출한 엔티티 간 관계를 그래프로 탐색하는 GraphRAG. 이 셋을 동시에 돌린다.\n이 하이브리드가 DataNexus와 특히 잘 맞는 이유가 있다. DataHub의 Glossary Term을 ApeRAG Entity Extraction의 Taxonomy로 주입하면 문서에서 추출된 엔티티가 자동으로 비즈니스 용어와 연결된다. Exact Match → Synonym Match → Fuzzy Match(임계값 0.85) → Context Match, 4단계 Entity Resolution을 거친다.\nMinerU 통합도 빠뜨릴 수 없다. 엔터프라이즈 문서에는 복잡한 테이블, 수식, 다단 레이아웃이 흔한데 일반 PDF 파서로는 테이블 행/열이 깨진다. 특히 사업보고서처럼 테이블 안에 병합 셀이 난무하는 문서는 파싱 결과가 처참하다. MinerU는 문서 구조를 보존하면서 파싱하기 때문에 이 문제를 정면으로 해결한다.\n그래프 DB 가장 큰 변수는 Neo4j 라이선스였다.\nCommunity Edition과 Enterprise Edition의 결정적 차이는 Multi-DB다. Community는 인스턴스 하나에 그래프 하나. Enterprise는 같은 인스턴스 안에서 여러 데이터베이스를 만들 수 있다.\nDataNexus에서 Multi-DB는 필수다. 그룹사별로 온톨로지 그래프를 격리해야 한다. groupA_ontology_db, groupB_ontology_db처럼 테넌트별 데이터베이스를 분리하고 사용자 권한으로 접근을 제어하는 구조. Community 단일 DB에 전부 넣고 라벨로 구분하는 건 보안상 말이 안 된다.\n그렇다고 Neo4j Enterprise 라이선스를 살 수는 없다. 오픈소스 프로젝트의 원칙에 어긋난다.\nDozerDB가 이 딜레마를 풀었다. Neo4j Community Edition 위에 Enterprise 기능을 얹는 오픈소스 플러그인인데 Multi-DB를 지원한다. CREATE DATABASE로 테넌트별 그래프를 만들 수 있고, Cypher 쿼리도 그대로 쓴다.\nArangoDB도 봤다. 멀티모델(문서 + 그래프 + 키밸류)이 매력적인데 Cypher를 쓸 수 없다. 자체 쿼리 언어 AQL이 그래프 탐색에는 괜찮지만 Neo4j 생태계의 라이브러리나 도구를 못 쓰게 된다. 온톨로지를 Cypher로 질의하는 패턴과 레퍼런스가 압도적으로 많기 때문에 생태계 호환성을 택했다.\nDozerDB의 한계도 안다. Fabric — 크로스 DB 쿼리 — 은 아직 미지원이라 서로 다른 데이터베이스를 한 번의 Cypher로 질의하는 건 불가능하다. Phase 3 이후로 미뤘다. 당장은 단일 테넌트 내 질의만으로 충분하다.\n네 개를 연결하면 네 가지를 나란히 놓으면 도구 네 개다. 연결해야 파이프라인이 된다.\nDataHub에서 Glossary Term이 바뀌면 Kafka MCL 이벤트가 발행된다. 이 이벤트가 DozerDB 온톨로지 그래프에 실시간 반영되고, 동시에 Vanna의 RAG Store에도 들어간다. NL2SQL 프롬프트에 주입되는 맥락이 자동 갱신되는 셈이다. ApeRAG의 Entity Extraction은 DataHub Glossary를 Taxonomy로 참조하니까 문서 검색 결과도 최신 용어 체계에 연결된다.\n한 곳에서 용어를 고치면 네 군데가 동시에 바뀐다. 메타데이터 변경을 수동으로 전파하는 방식은 규모가 커질수록 반드시 어딘가 누락된다. 이 파이프라인이 그 문제를 막는다.\n다음 글 DataHub의 Business Glossary를 온톨로지로 쓸 때의 한계와 우회를 다룬다.\nDataNexus를 설계하고 구축하는 과정을 기록합니다. GitHub | LinkedIn\n","permalink":"https://biz-agentic-ai.github.io/posts/datanexus/002-architecture-decisions/","summary":"DataNexus의 기술 스택을 DataHub + Vanna + ApeRAG + DozerDB로 결정한 과정. 후보군에서 탈락한 것들과 그 이유.","title":"2. 4개의 오픈소스를 이 조합으로 결정하기까지"},{"content":"\u0026ldquo;VIP 기준이 뭐죠?\u0026rdquo; 유통사 BI Agent 프로젝트에서 있었던 일이다.\n현업 담당자가 테스트 중에 Agent에게 물었다. \u0026ldquo;지난달 VIP 고객 매출 알려줘.\u0026rdquo; 시스템이 숫자를 뱉어냈는데, 담당자 표정이 좋지 않았다. \u0026ldquo;이거 뭔가 이상한데요. VIP 기준이 우리 팀이랑 다른 것 같아요.\u0026rdquo;\n마케팅의 VIP와 CRM의 VIP가 달랐다. 매출도 마찬가지. 순매출이냐 총매출이냐에 따라 수억 단위로 차이가 난다.\n처음 겪는 문제가 아니었다. DW를 클라우드로 옮기는 프로젝트에서도 봤고, 차세대 정보계를 여러 벤더와 1년 넘게 만들 때도 똑같았다. 벤더마다 \u0026ldquo;매출\u0026rdquo;, \u0026ldquo;원가\u0026quot;의 기준이 달라서 데이터 정합성 잡느라 몇 주씩 지연됐다. 용어 하나 안 맞으면 전체 일정이 밀린다. DW/BI 프로젝트를 하면서 이 문제가 안 나온 적이 없다.\n엔터프라이즈 DW에는 테이블과 컬럼이 있다. 없는 건 맥락이다. \u0026ldquo;이 컬럼이 비즈니스에서 뭘 의미하는지\u0026quot;가 기계가 읽을 수 있는 형태로 어디에도 정의되어 있지 않다.\n그래서 직접 만들기로 했다.\nNL2SQL은 만능이 아니다 요즘 NL2SQL 도구가 많다. 자연어를 SQL로 바꿔주는 것 자체는 이미 된다.\n실환경에 붙여보면 얘기가 달라진다. 유통사 프로젝트에서 벤치마크 점수가 높은 모델을 실제 DW에 연결하니까 체감 정확도가 확 떨어졌다. 카드사·통신사·공공데이터까지 내외부 데이터를 통합해 놓은 환경이었다. LLM이 이 복잡도(테이블 구조와 질문의 난이도)를 감당하지 못했다.\nDW의 DDL을 열어보면 원인이 보인다. T_CUST_MST.CUST_GRD_CD, T_ORD_DTL.SALE_AMT 같은 약어 테이블이 수천 개다. 벤치마크 DB의 customer_name, order_date와는 차원이 다르다. 같은 회사인데도 사업부마다 테이블 네이밍 규칙이 다르고, \u0026ldquo;매출\u0026quot;이라는 단어 하나가 사업부마다 다른 테이블을 가리킨다.\n파생 지표는 더 골치다. \u0026ldquo;순매출\u0026quot;은 단일 컬럼이 아니다. SUM(SALE_AMT) - SUM(RTN_AMT) - SUM(DC_AMT) 같은 계산식인데, 이 공식은 어떤 DDL에도 안 적혀 있다. 현업 머릿속에 있거나, 잘해야 누군가의 Excel 정의서 어딘가에 묻혀 있다.\nNL2SQL의 병목은 SQL 생성 능력이 아니다. 맥락이 없다.\n온톨로지라는 선택 대화형 BI에서 프롬프트 엔지니어링을 아무리 최적화해도 DDL만으로는 한계가 뚜렷했다. 멀티에이전트 아키텍처도 설계해 봤는데, 근본 문제는 같았다. LLM에게 줄 맥락 자체가 없다.\n온톨로지를 붙이기로 했다.\n거창한 얘기가 아니다. 실용적으로 보면 이런 것이다:\n# 순매출 용어 정의 - term: 순매출 definition: 총매출에서 반품과 에누리를 차감한 금액 formula: SUM(SALE_AMT) - SUM(RTN_AMT) - SUM(DC_AMT) synonyms: [Net Sales, 순매출액, 넷세일즈] related_tables: [T_SALE_DTL, T_RTN_DTL] owner: 재무팀 이런 용어 정의를 메타데이터 카탈로그에 등록하고, NL2SQL 엔진의 RAG Store에 자동 동기화한다. LLM이 \u0026ldquo;순매출\u0026quot;이라는 단어를 봤을 때 어떤 테이블의 어떤 컬럼을 어떤 계산식으로 조합해야 하는지 알게 되는 구조다.\n처리 흐름은 이렇다:\n온톨로지 적용 전후 차이 — 내부 목표치가 EX(Execution Accuracy) 기준 +15~20%p 향상이다. MVP에서 EX 80% 이상, 안정화 단계에서 90% 이상. 현실적인 수치인지는 만들어 보면서 검증한다.\nDDL 붙여넣기로는 안 되는 이유 엔터프라이즈 DW의 DDL을 다 붙여넣으면 수만수십만 토큰이다. 테이블이 수백수천 개인 환경에서는 컨텍스트 윈도우에 다 못 넣는다. 넣더라도 LLM이 그 안에서 정확한 테이블을 골라내는 건 Needle-in-a-Haystack 문제다.\n보안도 걸린다. 기업 내부 스키마를 외부 API에 통째로 보낼 수 없다. 같은 \u0026ldquo;매출\u0026quot;이라도 A그룹사 사용자와 B그룹사 사용자가 봐야 하는 범위가 다르다. Row-level Security가 필요한 환경이다.\n가장 큰 문제는 지속성이다. DDL은 바뀐다. 비즈니스 용어 정의도 바뀐다. 차세대 정보계를 오픈하고 나서도 단계별 릴리즈가 이어지면 매번 메타데이터가 바뀐다. 일회성 프롬프트가 아니라, 변경을 감지해서 자동으로 RAG Store를 갱신하는 파이프라인이 필요하다.\n따로 플랫폼을 만들어야겠다고 생각했다.\nDataNexus가 하려는 것 네 가지 컴포넌트로 구성된다.\n메타데이터 카탈로그 — 비즈니스 용어 정의, 테이블 메타, 데이터 계보를 한 곳에서 관리한다. 온톨로지의 원천(Source of Truth). NL2SQL 엔진 — 자연어를 SQL로 변환하되, 온톨로지에서 가져온 맥락을 프롬프트에 주입한다. DDL만 던져주는 방식과 정확도 차이가 확 난다. 문서 지식엔진 — 사업보고서, 정책문서 같은 비정형 데이터를 GraphRAG + 벡터 하이브리드로 검색한다. 그래프 DB — 온톨로지를 지식 그래프로 저장한다. 그룹사별 Multi-DB 격리까지. 카탈로그에서 정의한 온톨로지가 NL2SQL과 문서검색에 자동 동기화되어, 사용자의 자연어 질문에 맥락을 입혀주는 구조다. 각 컴포넌트에 사용한 오픈소스와 선정 이유는 다음 글에서 다룬다.\n왜 지금인가 LLM이 나오면서 판이 바뀌었다. 범용 모델은 빠르게 좋아지고 있고, 단순 기획이나 문서 생성은 곧 commodity가 된다. 차이를 만들려면 기업 데이터의 맥락을 구조화해서 모델에 주입할 수 있는 시스템이 있어야 한다.\n우리 회사의 \u0026ldquo;순매출\u0026rdquo; 정의가 뭔지, LLM이 아무리 똑똑해져도 모른다. 기업 내부에만 있는 지식이기 때문이다.\n이건 LLM 연구에서 말하는 \u0026ldquo;Non-verifiable Domain\u0026rdquo; 에 해당한다. 수학이나 코딩은 정답을 자동 검증할 수 있지만 기업 내부의 암묵적 지식, 역할별 해석 차이, 비공개 운영 데이터는 그렇지 않다. 외부에서 즉시 판별하기 어려운 영역. 이런 데이터 위에 쌓는 경쟁 우위를 AI 전략에서는 \u0026ldquo;Data Moat\u0026rdquo; 라고 부른다.\n이 우위가 영구적이라고는 생각하지 않는다. 범용 모델의 일반화 속도를 DataNexus의 데이터 축적 속도가 앞서야 한다. 시간이 중요하다.\nData Moat를 쌓는 방법은 이렇다:\n온톨로지 기반 맥락 — 도메인 전문가가 용어를 정제할수록 두꺼워지는 메타데이터 카탈로그 역할별 해석 — 같은 질문이라도 재무팀과 마케팅팀에 다른 응답을 주는 페르소나 최적화. 사용 패턴이 쌓일수록 개인화된다. 시간축 지식 그래프 — Temporal Knowledge Graph로 \u0026ldquo;작년 4분기 기준 VIP 정의\u0026quot;와 \u0026ldquo;올해 기준 VIP 정의\u0026quot;를 구분 비공개 데이터 자산 — 그룹사별 그래프 DB 격리 + Row-level Security. 각 그룹사의 데이터가 독립 자산이 된다. 올해 상반기까지 MVP를 내고 데이터 축적 루프를 돌리는 게 목표다.\n이 블로그의 목적 DataNexus를 만들면서 부딪히는 의사결정, 삽질, 해결 과정을 기록한다.\n다룰 것들:\n기술 스택을 선정한 과정 (후보군 탈락 사유 포함) 메타데이터 카탈로그의 Business Glossary를 온톨로지로 쓸 때의 한계와 우회 SKOS 호환 레이어를 넣은 이유 NL2SQL 엔진의 User-Aware 설계와 Row-level Security CQ(Competency Questions)로 온톨로지를 사전 검증하는 법 Query Router에서 결정론적 vs 확률론적 라우팅을 나누는 기준 에이전트 태스크를 쪼개는 79% Rule 이론보다는 실제로 부딪힌 문제와 그걸 어떻게 풀었는지 (또는 아직 못 풀었는지)를 쓸 예정이다.\n다음 글 DataNexus의 기술 스택 — 4개의 오픈소스를 이 조합으로 결정하기까지의 과정. 후보군에서 탈락한 것들과 그 이유를 정리한다.\nDataNexus를 설계하고 구축하는 과정을 기록합니다. GitHub | LinkedIn\n","permalink":"https://biz-agentic-ai.github.io/posts/datanexus/001-why-datanexus/","summary":"\u003ch2 id=\"vip-기준이-뭐죠\"\u003e\u0026ldquo;VIP 기준이 뭐죠?\u0026rdquo;\u003c/h2\u003e\n\u003cp\u003e유통사 BI Agent 프로젝트에서 있었던 일이다.\u003c/p\u003e\n\u003cp\u003e현업 담당자가 테스트 중에 Agent에게 물었다. \u0026ldquo;지난달 VIP 고객 매출 알려줘.\u0026rdquo; 시스템이 숫자를 뱉어냈는데, 담당자 표정이 좋지 않았다. \u0026ldquo;이거 뭔가 이상한데요. VIP 기준이 우리 팀이랑 다른 것 같아요.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003e마케팅의 VIP와 CRM의 VIP가 달랐다. 매출도 마찬가지. 순매출이냐 총매출이냐에 따라 수억 단위로 차이가 난다.\u003c/p\u003e\n\u003cp\u003e처음 겪는 문제가 아니었다. DW를 클라우드로 옮기는 프로젝트에서도 봤고, 차세대 정보계를 여러 벤더와 1년 넘게 만들 때도 똑같았다. 벤더마다 \u0026ldquo;매출\u0026rdquo;, \u0026ldquo;원가\u0026quot;의 기준이 달라서 데이터 정합성 잡느라 몇 주씩 지연됐다. 용어 하나 안 맞으면 전체 일정이 밀린다. DW/BI 프로젝트를 하면서 이 문제가 안 나온 적이 없다.\u003c/p\u003e","title":"1. 왜 DataNexus를 만드는가"},{"content":"Junho Lee (이준호) Data \u0026amp; AI Platform Architect | PM\nDW/BI 현장에서 대규모 DW 클라우드 전환부터 차세대 정보계 구축까지 설계하고 리드해왔습니다. Web/ERP 개발자로 시작해서 DW/BI 엔지니어, Technical Lead, 컨설팅 본부장을 거쳤고, 지금은 온톨로지 기반 AI 데이터 플랫폼을 만들고 있습니다.\n경력 요약 커리어 전반부는 엔터프라이즈 DW/BI에 집중했습니다. 대용량 DW의 클라우드 전환을 Tech Leader로 수행했고, 차세대 정보계 프로젝트를 멀티벤더 PMO로 운영했습니다. 소매·통신·제조·건설 등 산업 도메인을 가리지 않고 프로젝트를 수행해왔습니다.\n컨설팅 조직을 빌딩한 경험도 있습니다. 소수로 시작한 팀을 20명 규모까지 키우고, 매출도 수배 이상 성장시켰습니다. 채용, 교육, 기술 조직 운영, Presales, C레벨 대상 세미나까지 - 기술만 하는 사람은 아닙니다.\n최근에는 데이터와 AI의 접점에서 일하고 있습니다. LLM 기반 BI Agent를 구축하면서 NL2SQL의 실환경 한계를 경험했고, 그 과정에서 온톨로지 접근의 필요성을 확신하게 됐습니다. 지금은 DataNexus라는 통합 데이터 에이전트 플랫폼을 설계/구축 중입니다.\nDataNexus \u0026ldquo;Everyone is an Analyst.\u0026rdquo;\n엔터프라이즈 데이터 분석의 구조적 문제를 풀기 위한 플랫폼입니다. 자연어로 사내 데이터를 탐색하고 분석하는 AI 에이전트 — 말은 쉬운데, 실환경에서는 테이블명이 T_CUST_MST 같은 약어 투성이고, \u0026ldquo;순매출\u0026quot;이라는 단어 하나에도 부서마다 계산 로직이 다릅니다. DDL만으로는 LLM이 비즈니스 맥락을 이해할 수 없습니다.\nDataNexus는 온톨로지 기반 NL2SQL 엔진, GraphRAG, Data Catalog를 결합해서 이 문제를 풀어갑니다. 오픈소스를 조합한 아키텍처로, 비정형 문서와 정형 DB를 하나의 인터페이스로 다루는 구조입니다.\n이 블로그는 DataNexus를 만들어가는 과정을 기록합니다. 아키텍처 결정, 기술 선택의 이유, 삽질과 해결 과정을 있는 그대로 남깁니다.\n기술 영역 AI/ML — 온톨로지 LLM RAG, NL2SQL, Langchain, MCP, 멀티에이전트 시스템 설계 DW/Data Platform — Azure Synapse, BigQuery, Redshift, PostgreSQL, Oracle, Yellowbrick, Palantir Foundry BI — Power BI, Tableau, MicroStrategy, Qlik Sense, Looker, Superset ETL/ELT — ADF, SAP Data Services, IBM DataStage, Informatica, Databricks, SSIS Cloud — Azure (Synapse, ADF, ML), AWS (Redshift, S3, Glue), GCP (BigQuery, Gemini) Graph/Catalog — DataHub, Neo4j(DozerDB), ApeRAG Contact GitHub: @biz-agentic-ai LinkedIn: linkedin.com/in/leejuno ","permalink":"https://biz-agentic-ai.github.io/about/","summary":"이준호 - Data \u0026amp; AI Platform Architect","title":"About"}]